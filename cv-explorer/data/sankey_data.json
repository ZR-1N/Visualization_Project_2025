[{"year": 2015, "source": "recognition", "target": "cnn", "value": 116.86}, {"year": 2015, "source": "detection", "target": "cnn", "value": 104.14}, {"year": 2015, "source": "segmentation", "target": "cnn", "value": 28.07}, {"year": 2015, "source": "recognition", "target": "rnn", "value": 13.73}, {"year": 2015, "source": "detection", "target": "rnn", "value": 8.02}, {"year": 2015, "source": "recognition", "target": "mlp", "value": 7.78}, {"year": 2015, "source": "3d_vision", "target": "cnn", "value": 7.42}, {"year": 2015, "source": "restoration", "target": "cnn", "value": 6.83}, {"year": 2015, "source": "tracking", "target": "cnn", "value": 3.15}, {"year": 2015, "source": "generation", "target": "rnn", "value": 2.72}, {"year": 2016, "source": "recognition", "target": "cnn", "value": 215.87}, {"year": 2016, "source": "detection", "target": "cnn", "value": 81.94}, {"year": 2016, "source": "segmentation", "target": "cnn", "value": 72.62}, {"year": 2016, "source": "restoration", "target": "cnn", "value": 29.7}, {"year": 2016, "source": "tracking", "target": "cnn", "value": 23.01}, {"year": 2016, "source": "generation", "target": "cnn", "value": 16.3}, {"year": 2016, "source": "3d_vision", "target": "cnn", "value": 15.6}, {"year": 2016, "source": "recognition", "target": "rnn", "value": 11.38}, {"year": 2016, "source": "detection", "target": "rnn", "value": 10.19}, {"year": 2016, "source": "generation", "target": "rnn", "value": 8.15}, {"year": 2016, "source": "tracking", "target": "rnn", "value": 7.98}, {"year": 2016, "source": "segmentation", "target": "rnn", "value": 7.2}, {"year": 2016, "source": "generation", "target": "gan", "value": 5.55}, {"year": 2016, "source": "restoration", "target": "rnn", "value": 2.61}, {"year": 2016, "source": "segmentation", "target": "mlp", "value": 2.45}, {"year": 2017, "source": "recognition", "target": "cnn", "value": 335.07}, {"year": 2017, "source": "detection", "target": "cnn", "value": 211.89}, {"year": 2017, "source": "segmentation", "target": "cnn", "value": 117.74}, {"year": 2017, "source": "restoration", "target": "cnn", "value": 64.73}, {"year": 2017, "source": "3d_vision", "target": "cnn", "value": 63.71}, {"year": 2017, "source": "recognition", "target": "rnn", "value": 47.33}, {"year": 2017, "source": "segmentation", "target": "rnn", "value": 38.06}, {"year": 2017, "source": "recognition", "target": "gan", "value": 27.22}, {"year": 2017, "source": "generation", "target": "gan", "value": 26.56}, {"year": 2017, "source": "detection", "target": "rnn", "value": 20.49}, {"year": 2017, "source": "tracking", "target": "cnn", "value": 19.26}, {"year": 2017, "source": "tracking", "target": "rnn", "value": 18.54}, {"year": 2017, "source": "detection", "target": "gan", "value": 17.63}, {"year": 2017, "source": "generation", "target": "rnn", "value": 15.87}, {"year": 2017, "source": "restoration", "target": "gan", "value": 14.7}, {"year": 2017, "source": "generation", "target": "cnn", "value": 12.57}, {"year": 2017, "source": "recognition", "target": "mlp", "value": 8.74}, {"year": 2017, "source": "3d_vision", "target": "rnn", "value": 5.59}, {"year": 2017, "source": "restoration", "target": "rnn", "value": 3.33}, {"year": 2017, "source": "detection", "target": "gnn", "value": 3.08}, {"year": 2018, "source": "recognition", "target": "cnn", "value": 537.55}, {"year": 2018, "source": "detection", "target": "cnn", "value": 410.54}, {"year": 2018, "source": "segmentation", "target": "cnn", "value": 199.35}, {"year": 2018, "source": "generation", "target": "gan", "value": 161.21}, {"year": 2018, "source": "restoration", "target": "cnn", "value": 144.93}, {"year": 2018, "source": "recognition", "target": "rnn", "value": 81.16}, {"year": 2018, "source": "restoration", "target": "gan", "value": 62.26}, {"year": 2018, "source": "tracking", "target": "cnn", "value": 60.46}, {"year": 2018, "source": "recognition", "target": "gan", "value": 55.33}, {"year": 2018, "source": "3d_vision", "target": "cnn", "value": 53.06}, {"year": 2018, "source": "generation", "target": "cnn", "value": 34.31}, {"year": 2018, "source": "detection", "target": "rnn", "value": 29.97}, {"year": 2018, "source": "segmentation", "target": "rnn", "value": 27.57}, {"year": 2018, "source": "recognition", "target": "gnn", "value": 26.03}, {"year": 2018, "source": "detection", "target": "gan", "value": 22.42}, {"year": 2018, "source": "3d_vision", "target": "gan", "value": 17.16}, {"year": 2018, "source": "recognition", "target": "mlp", "value": 12.64}, {"year": 2018, "source": "segmentation", "target": "gan", "value": 12.03}, {"year": 2018, "source": "tracking", "target": "rnn", "value": 11.76}, {"year": 2018, "source": "generation", "target": "rnn", "value": 10.65}, {"year": 2018, "source": "restoration", "target": "rnn", "value": 7.55}, {"year": 2018, "source": "3d_vision", "target": "mlp", "value": 6.21}, {"year": 2018, "source": "tracking", "target": "gan", "value": 4.18}, {"year": 2018, "source": "recognition", "target": "transformer", "value": 3.89}, {"year": 2018, "source": "tracking", "target": "mlp", "value": 2.56}, {"year": 2018, "source": "detection", "target": "mlp", "value": 2.56}, {"year": 2018, "source": "3d_vision", "target": "rnn", "value": 2.01}, {"year": 2019, "source": "recognition", "target": "cnn", "value": 1123.89}, {"year": 2019, "source": "detection", "target": "cnn", "value": 835.99}, {"year": 2019, "source": "segmentation", "target": "cnn", "value": 558.35}, {"year": 2019, "source": "restoration", "target": "cnn", "value": 416.47}, {"year": 2019, "source": "generation", "target": "gan", "value": 195.15}, {"year": 2019, "source": "restoration", "target": "gan", "value": 179.83}, {"year": 2019, "source": "3d_vision", "target": "cnn", "value": 177.63}, {"year": 2019, "source": "tracking", "target": "cnn", "value": 140.52}, {"year": 2019, "source": "recognition", "target": "rnn", "value": 130.27}, {"year": 2019, "source": "recognition", "target": "gan", "value": 104.28}, {"year": 2019, "source": "generation", "target": "cnn", "value": 79.59}, {"year": 2019, "source": "segmentation", "target": "rnn", "value": 75.31}, {"year": 2019, "source": "detection", "target": "gan", "value": 54.63}, {"year": 2019, "source": "segmentation", "target": "gan", "value": 52.11}, {"year": 2019, "source": "detection", "target": "rnn", "value": 45.15}, {"year": 2019, "source": "restoration", "target": "rnn", "value": 42.03}, {"year": 2019, "source": "3d_vision", "target": "gan", "value": 36.61}, {"year": 2019, "source": "recognition", "target": "transformer", "value": 32.12}, {"year": 2019, "source": "recognition", "target": "contrastive", "value": 21.51}, {"year": 2019, "source": "3d_vision", "target": "rnn", "value": 20.67}, {"year": 2019, "source": "generation", "target": "rnn", "value": 16.21}, {"year": 2019, "source": "restoration", "target": "transformer", "value": 15.26}, {"year": 2019, "source": "3d_vision", "target": "gnn", "value": 14.76}, {"year": 2019, "source": "segmentation", "target": "transformer", "value": 13.43}, {"year": 2019, "source": "recognition", "target": "gnn", "value": 12.95}, {"year": 2019, "source": "tracking", "target": "rnn", "value": 12.93}, {"year": 2019, "source": "3d_vision", "target": "transformer", "value": 6.46}, {"year": 2019, "source": "detection", "target": "transformer", "value": 6.43}, {"year": 2019, "source": "generation", "target": "transformer", "value": 6.09}, {"year": 2019, "source": "recognition", "target": "mlp", "value": 5.42}, {"year": 2019, "source": "detection", "target": "gnn", "value": 5.26}, {"year": 2019, "source": "segmentation", "target": "gnn", "value": 4.66}, {"year": 2019, "source": "detection", "target": "contrastive", "value": 3.4}, {"year": 2019, "source": "restoration", "target": "mlp", "value": 2.22}, {"year": 2020, "source": "recognition", "target": "cnn", "value": 1190.66}, {"year": 2020, "source": "detection", "target": "cnn", "value": 1051.69}, {"year": 2020, "source": "segmentation", "target": "cnn", "value": 851.2}, {"year": 2020, "source": "restoration", "target": "cnn", "value": 452.33}, {"year": 2020, "source": "restoration", "target": "gan", "value": 323.95}, {"year": 2020, "source": "generation", "target": "gan", "value": 321.98}, {"year": 2020, "source": "3d_vision", "target": "cnn", "value": 226.61}, {"year": 2020, "source": "recognition", "target": "rnn", "value": 200.92}, {"year": 2020, "source": "detection", "target": "gan", "value": 142.63}, {"year": 2020, "source": "recognition", "target": "gan", "value": 140.65}, {"year": 2020, "source": "tracking", "target": "cnn", "value": 101.2}, {"year": 2020, "source": "segmentation", "target": "gan", "value": 88.75}, {"year": 2020, "source": "generation", "target": "cnn", "value": 75.06}, {"year": 2020, "source": "recognition", "target": "gnn", "value": 74.45}, {"year": 2020, "source": "detection", "target": "rnn", "value": 69.76}, {"year": 2020, "source": "segmentation", "target": "rnn", "value": 69.7}, {"year": 2020, "source": "recognition", "target": "transformer", "value": 68.8}, {"year": 2020, "source": "restoration", "target": "rnn", "value": 56.41}, {"year": 2020, "source": "3d_vision", "target": "gan", "value": 38.87}, {"year": 2020, "source": "generation", "target": "transformer", "value": 36.74}, {"year": 2020, "source": "recognition", "target": "contrastive", "value": 30.86}, {"year": 2020, "source": "detection", "target": "transformer", "value": 24.76}, {"year": 2020, "source": "segmentation", "target": "transformer", "value": 22.51}, {"year": 2020, "source": "tracking", "target": "rnn", "value": 21.94}, {"year": 2020, "source": "3d_vision", "target": "gnn", "value": 20.27}, {"year": 2020, "source": "segmentation", "target": "contrastive", "value": 19.65}, {"year": 2020, "source": "3d_vision", "target": "transformer", "value": 17.11}, {"year": 2020, "source": "detection", "target": "gnn", "value": 15.69}, {"year": 2020, "source": "segmentation", "target": "gnn", "value": 13.02}, {"year": 2020, "source": "generation", "target": "rnn", "value": 12.71}, {"year": 2020, "source": "3d_vision", "target": "rnn", "value": 9.1}, {"year": 2020, "source": "restoration", "target": "transformer", "value": 9.01}, {"year": 2020, "source": "detection", "target": "contrastive", "value": 8.74}, {"year": 2020, "source": "recognition", "target": "mlp", "value": 8.55}, {"year": 2020, "source": "restoration", "target": "vit", "value": 8.4}, {"year": 2020, "source": "3d_vision", "target": "nerf", "value": 8.36}, {"year": 2020, "source": "generation", "target": "vit", "value": 8.12}, {"year": 2020, "source": "tracking", "target": "gnn", "value": 7.67}, {"year": 2020, "source": "detection", "target": "vit", "value": 5.46}, {"year": 2020, "source": "detection", "target": "mlp", "value": 5.45}, {"year": 2020, "source": "generation", "target": "gnn", "value": 5.1}, {"year": 2020, "source": "generation", "target": "nerf", "value": 4.97}, {"year": 2020, "source": "3d_vision", "target": "vit", "value": 4.68}, {"year": 2020, "source": "segmentation", "target": "mlp", "value": 4.29}, {"year": 2020, "source": "tracking", "target": "transformer", "value": 4.26}, {"year": 2020, "source": "recognition", "target": "vit", "value": 3.77}, {"year": 2020, "source": "segmentation", "target": "vit", "value": 3.39}, {"year": 2020, "source": "restoration", "target": "mlp", "value": 2.25}, {"year": 2020, "source": "recognition", "target": "diffusion", "value": 2.17}, {"year": 2021, "source": "recognition", "target": "cnn", "value": 1273.25}, {"year": 2021, "source": "detection", "target": "cnn", "value": 1097.04}, {"year": 2021, "source": "segmentation", "target": "cnn", "value": 820.47}, {"year": 2021, "source": "restoration", "target": "cnn", "value": 425.05}, {"year": 2021, "source": "restoration", "target": "gan", "value": 311.7}, {"year": 2021, "source": "generation", "target": "gan", "value": 304.14}, {"year": 2021, "source": "3d_vision", "target": "cnn", "value": 220.77}, {"year": 2021, "source": "recognition", "target": "rnn", "value": 146.18}, {"year": 2021, "source": "segmentation", "target": "transformer", "value": 135.07}, {"year": 2021, "source": "recognition", "target": "gnn", "value": 132.4}, {"year": 2021, "source": "recognition", "target": "gan", "value": 126.19}, {"year": 2021, "source": "detection", "target": "gan", "value": 124.28}, {"year": 2021, "source": "recognition", "target": "transformer", "value": 120.49}, {"year": 2021, "source": "detection", "target": "transformer", "value": 119.75}, {"year": 2021, "source": "tracking", "target": "cnn", "value": 76.56}, {"year": 2021, "source": "recognition", "target": "contrastive", "value": 60.74}, {"year": 2021, "source": "3d_vision", "target": "nerf", "value": 56.33}, {"year": 2021, "source": "detection", "target": "rnn", "value": 55.47}, {"year": 2021, "source": "generation", "target": "cnn", "value": 52.59}, {"year": 2021, "source": "3d_vision", "target": "transformer", "value": 51.18}, {"year": 2021, "source": "segmentation", "target": "gan", "value": 50.6}, {"year": 2021, "source": "segmentation", "target": "rnn", "value": 49.41}, {"year": 2021, "source": "restoration", "target": "transformer", "value": 47.14}, {"year": 2021, "source": "3d_vision", "target": "gnn", "value": 42.14}, {"year": 2021, "source": "restoration", "target": "rnn", "value": 39.71}, {"year": 2021, "source": "segmentation", "target": "gnn", "value": 38.8}, {"year": 2021, "source": "3d_vision", "target": "gan", "value": 35.16}, {"year": 2021, "source": "recognition", "target": "mlp", "value": 30.89}, {"year": 2021, "source": "detection", "target": "vit", "value": 25.38}, {"year": 2021, "source": "generation", "target": "nerf", "value": 23.04}, {"year": 2021, "source": "generation", "target": "transformer", "value": 22.33}, {"year": 2021, "source": "3d_vision", "target": "mlp", "value": 21.56}, {"year": 2021, "source": "detection", "target": "contrastive", "value": 20.98}, {"year": 2021, "source": "recognition", "target": "vit", "value": 18.99}, {"year": 2021, "source": "segmentation", "target": "contrastive", "value": 18.33}, {"year": 2021, "source": "detection", "target": "gnn", "value": 17.8}, {"year": 2021, "source": "tracking", "target": "rnn", "value": 14.78}, {"year": 2021, "source": "3d_vision", "target": "rnn", "value": 13.59}, {"year": 2021, "source": "generation", "target": "rnn", "value": 13.58}, {"year": 2021, "source": "restoration", "target": "gnn", "value": 10.35}, {"year": 2021, "source": "3d_vision", "target": "diffusion", "value": 9.11}, {"year": 2021, "source": "tracking", "target": "gnn", "value": 8.99}, {"year": 2021, "source": "segmentation", "target": "mlp", "value": 7.6}, {"year": 2021, "source": "generation", "target": "contrastive", "value": 7.28}, {"year": 2021, "source": "detection", "target": "mlp", "value": 7.14}, {"year": 2021, "source": "tracking", "target": "transformer", "value": 6.84}, {"year": 2021, "source": "restoration", "target": "vit", "value": 5.76}, {"year": 2021, "source": "segmentation", "target": "vit", "value": 5.36}, {"year": 2021, "source": "3d_vision", "target": "vit", "value": 4.72}, {"year": 2021, "source": "3d_vision", "target": "contrastive", "value": 4.0}, {"year": 2021, "source": "generation", "target": "gnn", "value": 3.8}, {"year": 2021, "source": "generation", "target": "mlp", "value": 3.74}, {"year": 2021, "source": "segmentation", "target": "diffusion", "value": 2.79}, {"year": 2021, "source": "restoration", "target": "mlp", "value": 2.68}, {"year": 2021, "source": "tracking", "target": "contrastive", "value": 2.25}, {"year": 2021, "source": "tracking", "target": "gan", "value": 2.08}, {"year": 2022, "source": "recognition", "target": "cnn", "value": 1320.64}, {"year": 2022, "source": "detection", "target": "cnn", "value": 1195.17}, {"year": 2022, "source": "segmentation", "target": "cnn", "value": 817.72}, {"year": 2022, "source": "restoration", "target": "cnn", "value": 400.09}, {"year": 2022, "source": "detection", "target": "transformer", "value": 361.26}, {"year": 2022, "source": "restoration", "target": "gan", "value": 341.46}, {"year": 2022, "source": "segmentation", "target": "transformer", "value": 338.76}, {"year": 2022, "source": "recognition", "target": "transformer", "value": 325.51}, {"year": 2022, "source": "generation", "target": "gan", "value": 268.25}, {"year": 2022, "source": "3d_vision", "target": "cnn", "value": 238.44}, {"year": 2022, "source": "detection", "target": "gan", "value": 152.83}, {"year": 2022, "source": "recognition", "target": "gnn", "value": 123.21}, {"year": 2022, "source": "recognition", "target": "contrastive", "value": 115.36}, {"year": 2022, "source": "3d_vision", "target": "transformer", "value": 112.73}, {"year": 2022, "source": "restoration", "target": "transformer", "value": 110.43}, {"year": 2022, "source": "recognition", "target": "rnn", "value": 103.11}, {"year": 2022, "source": "segmentation", "target": "gan", "value": 92.37}, {"year": 2022, "source": "detection", "target": "rnn", "value": 91.8}, {"year": 2022, "source": "3d_vision", "target": "nerf", "value": 83.01}, {"year": 2022, "source": "tracking", "target": "cnn", "value": 69.4}, {"year": 2022, "source": "recognition", "target": "vit", "value": 67.62}, {"year": 2022, "source": "recognition", "target": "gan", "value": 64.47}, {"year": 2022, "source": "generation", "target": "cnn", "value": 62.53}, {"year": 2022, "source": "detection", "target": "contrastive", "value": 60.07}, {"year": 2022, "source": "segmentation", "target": "contrastive", "value": 51.13}, {"year": 2022, "source": "generation", "target": "transformer", "value": 49.27}, {"year": 2022, "source": "3d_vision", "target": "gnn", "value": 46.99}, {"year": 2022, "source": "tracking", "target": "transformer", "value": 46.51}, {"year": 2022, "source": "detection", "target": "vit", "value": 41.73}, {"year": 2022, "source": "3d_vision", "target": "gan", "value": 39.89}, {"year": 2022, "source": "detection", "target": "gnn", "value": 33.45}, {"year": 2022, "source": "restoration", "target": "rnn", "value": 31.56}, {"year": 2022, "source": "segmentation", "target": "gnn", "value": 31.13}, {"year": 2022, "source": "3d_vision", "target": "rnn", "value": 29.1}, {"year": 2022, "source": "generation", "target": "contrastive", "value": 29.02}, {"year": 2022, "source": "segmentation", "target": "rnn", "value": 27.91}, {"year": 2022, "source": "restoration", "target": "diffusion", "value": 25.53}, {"year": 2022, "source": "segmentation", "target": "vit", "value": 21.14}, {"year": 2022, "source": "restoration", "target": "contrastive", "value": 21.07}, {"year": 2022, "source": "3d_vision", "target": "mlp", "value": 19.66}, {"year": 2022, "source": "recognition", "target": "mlp", "value": 19.18}, {"year": 2022, "source": "3d_vision", "target": "vit", "value": 19.17}, {"year": 2022, "source": "tracking", "target": "rnn", "value": 15.24}, {"year": 2022, "source": "3d_vision", "target": "contrastive", "value": 14.58}, {"year": 2022, "source": "segmentation", "target": "mlp", "value": 12.02}, {"year": 2022, "source": "generation", "target": "diffusion", "value": 11.7}, {"year": 2022, "source": "detection", "target": "mlp", "value": 11.44}, {"year": 2022, "source": "generation", "target": "nerf", "value": 11.31}, {"year": 2022, "source": "restoration", "target": "vit", "value": 8.54}, {"year": 2022, "source": "restoration", "target": "gnn", "value": 5.68}, {"year": 2022, "source": "tracking", "target": "contrastive", "value": 5.66}, {"year": 2022, "source": "generation", "target": "vit", "value": 5.38}, {"year": 2022, "source": "generation", "target": "rnn", "value": 4.84}, {"year": 2022, "source": "segmentation", "target": "diffusion", "value": 4.42}, {"year": 2022, "source": "generation", "target": "mlp", "value": 3.17}, {"year": 2022, "source": "restoration", "target": "mlp", "value": 2.79}, {"year": 2022, "source": "3d_vision", "target": "diffusion", "value": 2.24}, {"year": 2023, "source": "detection", "target": "cnn", "value": 1140.64}, {"year": 2023, "source": "recognition", "target": "cnn", "value": 1114.24}, {"year": 2023, "source": "segmentation", "target": "cnn", "value": 815.75}, {"year": 2023, "source": "detection", "target": "transformer", "value": 682.15}, {"year": 2023, "source": "segmentation", "target": "transformer", "value": 494.88}, {"year": 2023, "source": "restoration", "target": "cnn", "value": 409.25}, {"year": 2023, "source": "restoration", "target": "gan", "value": 398.35}, {"year": 2023, "source": "recognition", "target": "transformer", "value": 367.0}, {"year": 2023, "source": "generation", "target": "gan", "value": 338.31}, {"year": 2023, "source": "3d_vision", "target": "nerf", "value": 322.19}, {"year": 2023, "source": "restoration", "target": "transformer", "value": 293.45}, {"year": 2023, "source": "generation", "target": "diffusion", "value": 211.48}, {"year": 2023, "source": "recognition", "target": "contrastive", "value": 182.31}, {"year": 2023, "source": "3d_vision", "target": "transformer", "value": 177.54}, {"year": 2023, "source": "restoration", "target": "diffusion", "value": 155.47}, {"year": 2023, "source": "3d_vision", "target": "cnn", "value": 117.77}, {"year": 2023, "source": "recognition", "target": "rnn", "value": 112.72}, {"year": 2023, "source": "recognition", "target": "gnn", "value": 111.46}, {"year": 2023, "source": "recognition", "target": "vit", "value": 104.84}, {"year": 2023, "source": "detection", "target": "contrastive", "value": 101.11}, {"year": 2023, "source": "tracking", "target": "transformer", "value": 98.62}, {"year": 2023, "source": "segmentation", "target": "contrastive", "value": 97.71}, {"year": 2023, "source": "detection", "target": "gan", "value": 95.79}, {"year": 2023, "source": "recognition", "target": "gan", "value": 92.62}, {"year": 2023, "source": "detection", "target": "rnn", "value": 83.4}, {"year": 2023, "source": "segmentation", "target": "vit", "value": 81.94}, {"year": 2023, "source": "detection", "target": "vit", "value": 80.71}, {"year": 2023, "source": "restoration", "target": "rnn", "value": 75.55}, {"year": 2023, "source": "tracking", "target": "cnn", "value": 70.36}, {"year": 2023, "source": "generation", "target": "transformer", "value": 67.32}, {"year": 2023, "source": "segmentation", "target": "gan", "value": 62.84}, {"year": 2023, "source": "generation", "target": "nerf", "value": 62.4}, {"year": 2023, "source": "generation", "target": "cnn", "value": 59.04}, {"year": 2023, "source": "3d_vision", "target": "gan", "value": 49.36}, {"year": 2023, "source": "restoration", "target": "vit", "value": 41.99}, {"year": 2023, "source": "3d_vision", "target": "gnn", "value": 40.14}, {"year": 2023, "source": "segmentation", "target": "rnn", "value": 36.85}, {"year": 2023, "source": "detection", "target": "gnn", "value": 32.97}, {"year": 2023, "source": "restoration", "target": "contrastive", "value": 29.5}, {"year": 2023, "source": "segmentation", "target": "diffusion", "value": 28.02}, {"year": 2023, "source": "3d_vision", "target": "rnn", "value": 27.1}, {"year": 2023, "source": "3d_vision", "target": "contrastive", "value": 26.35}, {"year": 2023, "source": "generation", "target": "contrastive", "value": 25.41}, {"year": 2023, "source": "3d_vision", "target": "mlp", "value": 25.31}, {"year": 2023, "source": "recognition", "target": "mlp", "value": 23.31}, {"year": 2023, "source": "3d_vision", "target": "vit", "value": 20.29}, {"year": 2023, "source": "segmentation", "target": "gnn", "value": 18.7}, {"year": 2023, "source": "restoration", "target": "mlp", "value": 18.36}, {"year": 2023, "source": "restoration", "target": "gnn", "value": 18.04}, {"year": 2023, "source": "detection", "target": "diffusion", "value": 17.41}, {"year": 2023, "source": "3d_vision", "target": "diffusion", "value": 17.16}, {"year": 2023, "source": "segmentation", "target": "mlp", "value": 16.42}, {"year": 2023, "source": "recognition", "target": "diffusion", "value": 15.31}, {"year": 2023, "source": "tracking", "target": "rnn", "value": 12.81}, {"year": 2023, "source": "generation", "target": "rnn", "value": 12.5}, {"year": 2023, "source": "restoration", "target": "nerf", "value": 12.33}, {"year": 2023, "source": "tracking", "target": "vit", "value": 11.01}, {"year": 2023, "source": "generation", "target": "vit", "value": 10.27}, {"year": 2023, "source": "tracking", "target": "gnn", "value": 10.15}, {"year": 2023, "source": "tracking", "target": "contrastive", "value": 9.42}, {"year": 2023, "source": "detection", "target": "mlp", "value": 8.28}, {"year": 2023, "source": "generation", "target": "gnn", "value": 4.79}, {"year": 2023, "source": "generation", "target": "mlp", "value": 4.55}, {"year": 2023, "source": "tracking", "target": "gan", "value": 3.4}, {"year": 2023, "source": "segmentation", "target": "nerf", "value": 3.14}, {"year": 2023, "source": "tracking", "target": "nerf", "value": 2.61}, {"year": 2024, "source": "recognition", "target": "cnn", "value": 476.59}, {"year": 2024, "source": "detection", "target": "cnn", "value": 397.98}, {"year": 2024, "source": "detection", "target": "transformer", "value": 339.68}, {"year": 2024, "source": "segmentation", "target": "cnn", "value": 277.7}, {"year": 2024, "source": "3d_vision", "target": "nerf", "value": 272.92}, {"year": 2024, "source": "restoration", "target": "transformer", "value": 236.79}, {"year": 2024, "source": "segmentation", "target": "transformer", "value": 231.56}, {"year": 2024, "source": "recognition", "target": "transformer", "value": 215.27}, {"year": 2024, "source": "generation", "target": "diffusion", "value": 190.16}, {"year": 2024, "source": "restoration", "target": "cnn", "value": 178.99}, {"year": 2024, "source": "restoration", "target": "diffusion", "value": 168.47}, {"year": 2024, "source": "generation", "target": "gan", "value": 139.15}, {"year": 2024, "source": "restoration", "target": "gan", "value": 123.56}, {"year": 2024, "source": "3d_vision", "target": "transformer", "value": 95.43}, {"year": 2024, "source": "recognition", "target": "contrastive", "value": 76.13}, {"year": 2024, "source": "generation", "target": "nerf", "value": 63.02}, {"year": 2024, "source": "tracking", "target": "transformer", "value": 62.14}, {"year": 2024, "source": "generation", "target": "transformer", "value": 61.52}, {"year": 2024, "source": "segmentation", "target": "vit", "value": 59.36}, {"year": 2024, "source": "recognition", "target": "vit", "value": 58.57}, {"year": 2024, "source": "3d_vision", "target": "cnn", "value": 58.33}, {"year": 2024, "source": "detection", "target": "contrastive", "value": 55.09}, {"year": 2024, "source": "recognition", "target": "rnn", "value": 52.13}, {"year": 2024, "source": "detection", "target": "diffusion", "value": 47.21}, {"year": 2024, "source": "recognition", "target": "gnn", "value": 46.66}, {"year": 2024, "source": "detection", "target": "vit", "value": 46.53}, {"year": 2024, "source": "segmentation", "target": "contrastive", "value": 46.06}, {"year": 2024, "source": "recognition", "target": "gan", "value": 37.27}, {"year": 2024, "source": "3d_vision", "target": "gnn", "value": 33.53}, {"year": 2024, "source": "segmentation", "target": "gan", "value": 31.36}, {"year": 2024, "source": "detection", "target": "gan", "value": 30.23}, {"year": 2024, "source": "segmentation", "target": "gnn", "value": 27.13}, {"year": 2024, "source": "restoration", "target": "contrastive", "value": 26.51}, {"year": 2024, "source": "3d_vision", "target": "contrastive", "value": 24.13}, {"year": 2024, "source": "3d_vision", "target": "vit", "value": 24.0}, {"year": 2024, "source": "generation", "target": "cnn", "value": 23.65}, {"year": 2024, "source": "restoration", "target": "vit", "value": 22.94}, {"year": 2024, "source": "3d_vision", "target": "mlp", "value": 22.1}, {"year": 2024, "source": "segmentation", "target": "diffusion", "value": 22.0}, {"year": 2024, "source": "generation", "target": "contrastive", "value": 21.9}, {"year": 2024, "source": "tracking", "target": "cnn", "value": 21.77}, {"year": 2024, "source": "segmentation", "target": "mlp", "value": 19.71}, {"year": 2024, "source": "3d_vision", "target": "diffusion", "value": 18.8}, {"year": 2024, "source": "3d_vision", "target": "gan", "value": 17.65}, {"year": 2024, "source": "restoration", "target": "rnn", "value": 16.77}, {"year": 2024, "source": "detection", "target": "rnn", "value": 14.91}, {"year": 2024, "source": "generation", "target": "vit", "value": 14.7}, {"year": 2024, "source": "generation", "target": "rnn", "value": 12.89}, {"year": 2024, "source": "recognition", "target": "diffusion", "value": 11.56}, {"year": 2024, "source": "detection", "target": "gnn", "value": 9.59}, {"year": 2024, "source": "tracking", "target": "contrastive", "value": 8.68}, {"year": 2024, "source": "3d_vision", "target": "rnn", "value": 8.62}, {"year": 2024, "source": "tracking", "target": "rnn", "value": 8.17}, {"year": 2024, "source": "tracking", "target": "gnn", "value": 7.78}, {"year": 2024, "source": "segmentation", "target": "nerf", "value": 7.06}, {"year": 2024, "source": "restoration", "target": "mlp", "value": 5.76}, {"year": 2024, "source": "segmentation", "target": "rnn", "value": 4.79}, {"year": 2024, "source": "tracking", "target": "diffusion", "value": 3.91}, {"year": 2024, "source": "tracking", "target": "vit", "value": 3.62}, {"year": 2024, "source": "restoration", "target": "gnn", "value": 3.54}, {"year": 2024, "source": "recognition", "target": "mlp", "value": 2.55}, {"year": 2024, "source": "restoration", "target": "nerf", "value": 2.33}, {"year": 2024, "source": "generation", "target": "mlp", "value": 2.19}]