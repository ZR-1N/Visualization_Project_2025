import os
import time
import random
import json
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS

# åˆå§‹åŒ– Flask åº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# æ¨¡æ‹Ÿçš„ API Key é…ç½® (å®é™…ä½¿ç”¨æ—¶åº”ä»ç¯å¢ƒå˜é‡è·å–)
API_KEYS = {
    "deepseek": os.environ.get("DEEPSEEK_API_KEY", "sk-9894b47b4c8642aebaccc6756ccbe490"),
    "chatgpt": os.environ.get("OPENAI_API_KEY", ""),
    "gemini": os.environ.get("GEMINI_API_KEY", ""),
    "doubao": os.environ.get("DOUBAO_API_KEY", "")
}

# æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨


def generate_mock_response(text, context, prompt_type=None):
    if prompt_type == "scholar_profile":
        name = text
        desc = context.get("desc") or context.get("summary") or "è¿™ä½å­¦è€…ä»¥è®¡ç®—æœºè§†è§‰ç ”ç©¶é—»åã€‚"
        view_mode = context.get("leaderboardView")
        if view_mode == "nankai":
            return {
                "summary": (
                    f"**{name}** æ˜¯å—å¼€å¤§å­¦åª’ä½“è®¡ç®—å›¢é˜Ÿçš„ä¸­åšåŠ›é‡ã€‚\n\n"
                    f"1. **ğŸ“ NKU ä½¿å‘½**ï¼šæ·±è€•å—å¼€è®¡ç®—æœºè§†è§‰æ–¹å‘ï¼Œ{desc}\n"
                    "2. **ğŸ§  æ ¸å¿ƒè´¡çŒ®**ï¼šä»¥é¢†å…ˆçš„è§†è§‰æ¨¡å‹ä¸è¯„ä»·ä½“ç³»ï¼Œæ ‘ç«‹ NKU åœ¨å›½é™… CV ç¤¾ç¾¤çš„è¾¨è¯†åº¦ã€‚\n"
                    "3. **ğŸ›ï¸ é˜µåˆ—æ°”è´¨**ï¼šä»£è¡¨äº†å—å¼€è§†è§‰ç ”ç©¶åŠ›é‡çš„è¿›å–ç²¾ç¥ï¼Œæ˜¯é’å¹´å­¦è€…çš„æ ‡æ†ã€‚ (Mock Mode)"
                ),
                "keywords": ["NKU", "Computer Vision", "Media Lab"],
                "confidence": 0.95
            }
        return {
            "summary": f"**{name}** æ˜¯ AI é¢†åŸŸçš„ä¼ å¥‡äººç‰©ã€‚\n\n1. **ğŸ‘‘ å°ç¥ç†ç”±**ï¼šä»–æ˜¯æ·±åº¦å­¦ä¹ é©å‘½çš„å¥ åŸºäººä¹‹ä¸€ï¼Œå›¾çµå¥–å¾—ä¸»ã€‚\n2. **ğŸ§  æ ¸å¿ƒè´¡çŒ®**ï¼š{desc}.\n3. **ğŸŒŸ å†å²åœ°ä½**ï¼šä»–åœ¨ AI å‘å±•é•¿æ²³ä¸­ä¸ä»…æ˜¯å…ˆé©±ï¼Œæ›´æ˜¯ç²¾ç¥é¢†è¢–ã€‚ (Mock Mode)",
            "keywords": ["Deep Learning", "Turing Award", "AI Safety"],
            "confidence": 0.95
        }
    elif prompt_type == "paper_impact":
        return {
            "summary": f"**ã€Š{text}ã€‹** æ˜¯è®¡ç®—æœºè§†è§‰çš„é‡Œç¨‹ç¢‘ã€‚\n\n1. **ğŸ’¥ ç ´å±€ç‚¹**ï¼šè§£å†³äº†æ·±åº¦ç¥ç»ç½‘ç»œéšç€å±‚æ•°å¢åŠ è€Œæ— æ³•è®­ç»ƒçš„æ­»èƒ¡åŒã€‚\n2. **ğŸ”‘ æ ¸å¿ƒé­”æ³•**ï¼šå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ï¼Œè®©æ¢¯åº¦å¯ä»¥é¡ºç•…æµåŠ¨ã€‚\n3. **ğŸŒ ä¸–ç•Œå›å“**ï¼šæˆä¸ºäº†æ‰€æœ‰ç°ä»£è§†è§‰æ¨¡å‹çš„åŸºç¡€ç»„ä»¶ï¼Œå¼•å‘äº†æ·±å±‚ç½‘ç»œçš„ç ”ç©¶çƒ­æ½®ã€‚ (Mock Mode)",
            "keywords": ["ResNet", "Backbone", "Milestone"],
            "confidence": 0.98
        }

    templates = [
        f"å…³äº '{text}' çš„ç ”ç©¶åœ¨ {context.get('year', 'è¿‘æœŸ')} è¡¨ç°å‡ºæ˜¾è‘—çš„å¢é•¿è¶‹åŠ¿ã€‚",
        f"'{text}' æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ ¸å¿ƒè®®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ {context.get('venue', 'é¡¶çº§ä¼šè®®')} ä¸Šã€‚",
        f"ç»“åˆä¸Šä¸‹æ–‡åˆ†æï¼Œ'{text}' é€šå¸¸ä¸ {context.get('related', 'æ·±åº¦å­¦ä¹ ')} æŠ€æœ¯ç»“åˆä½¿ç”¨ã€‚",
        f"è¯¥ä¸»é¢˜çš„å¼•ç”¨é‡è¾¾åˆ° {context.get('citations', 0)}ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨å­¦æœ¯ç•Œçš„é«˜å½±å“åŠ›ã€‚",
        f"æ·±åº¦è§£è¯»ï¼š'{text}' ä»£è¡¨äº† {context.get('year', 'å½“å‰')} è§†è§‰ç ”ç©¶çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚"
    ]

    # éšæœºç”Ÿæˆä¸€äº›å…³é”®è¯
    mock_keywords = ["Computer Vision", "Deep Learning",
                     "AI", "Trend", "Analysis", "Data", "Model"]
    selected_keywords = random.sample(mock_keywords, 3)

    return {
        "summary": random.choice(templates) + " (Mock Mode: è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºå“åº”ï¼Œæœªè¿æ¥çœŸå® API)",
        "keywords": selected_keywords,
        "confidence": round(random.uniform(0.8, 0.99), 2)
    }

# æ¨¡å‹è°ƒç”¨é€»è¾‘
# æ³¨æ„ï¼šä»¥ä¸‹å‡½æ•°åœ¨æ²¡æœ‰çœŸå® API Key çš„æƒ…å†µä¸‹ä¼šè¿”å›æ¨¡æ‹Ÿæˆ–é”™è¯¯ä¿¡æ¯


def call_deepseek(text, context, api_key, prompt_type=None):
    if not api_key:
        # å¦‚æœæ²¡æœ‰ Keyï¼Œä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œè¿”å›ä¸€ä¸ªå¸¦æ ‡è®°çš„ Mock æ•°æ®
        time.sleep(1)
        return generate_mock_response(text, context, prompt_type)

    # å®é™…è°ƒç”¨ DeepSeek API çš„ä»£ç ç¤ºä¾‹ (éœ€æ ¹æ®å®˜æ–¹æ–‡æ¡£è°ƒæ•´)
    try:
        url = "https://api.deepseek.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}",
                   "Content-Type": "application/json"}

        if prompt_type == "scholar_profile":
            name = text
            desc = context.get("desc", "")
            tags = ", ".join(context.get("concepts", []))
            view_mode = context.get("leaderboardView")
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªAIåäººå ‚è§£è¯´å‘˜ã€‚è¯·ç”¨ Markdown æ ¼å¼å›ç­”ï¼Œè¯­æ°”ä¸“ä¸šä¸”å¸¦æœ‰å´‡æ•¬æ„Ÿã€‚"
            if view_mode == "nankai":
                user_content = (
                    f"è¯·ä»‹ç»å—å¼€å¤§å­¦è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ°å‡ºå­¦è€… {name}ã€‚\n"
                    f"ä»–/å¥¹åœ¨å—å¼€å¤§å­¦åª’ä½“è®¡ç®—å›¢é˜Ÿï¼ˆNKU Media Labï¼‰ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚\n"
                    f"è¯·é‡ç‚¹è§£è¯»å…¶åœ¨ CV é¢†åŸŸçš„æ ¸å¿ƒå­¦æœ¯åœ°ä½ï¼Œä»¥åŠå¯¹ä»–/å¥¹æ‰€ä»£è¡¨çš„å—å¼€è§†è§‰ç ”ç©¶åŠ›é‡çš„è¯„ä»·ã€‚"
                )
            else:
                user_content = (
                    f"è¯·ä»‹ç»è®¡ç®—æœºç§‘å­¦å®¶ {name}ã€‚\n"
                    f"èƒŒæ™¯ä¿¡æ¯ï¼š{desc}\n"
                    f"è¯·ç”¨ Markdown æ ¼å¼å›ç­”ï¼š\n"
                    f"1. **ğŸ‘‘ å°ç¥ç†ç”±**ï¼šä¸€å¥è¯æ¦‚æ‹¬ä»–ä¸ºä»€ä¹ˆæ˜¯ Top çº§åˆ«ã€‚\n"
                    f"2. **ğŸ§  æ ¸å¿ƒè´¡çŒ®**ï¼šé€šä¿—è§£é‡Šä»–çš„ 1-2 ä¸ªä»£è¡¨ä½œï¼ˆå¦‚ {tags}ï¼‰ã€‚\n"
                    f"3. **ğŸŒŸ å†å²åœ°ä½**ï¼šä»–åœ¨ AI å‘å±•é•¿æ²³ä¸­çš„åæ ‡ã€‚\n"
                    f"å­—æ•° 250 å­—ä»¥å†…ï¼Œä¿æŒç®€æ´æœ‰åŠ›ã€‚"
                )
        elif prompt_type == "paper_impact":
            title = text
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯å²å­¦å®¶ã€‚è¯·ç”¨ Markdown æ ¼å¼è§£è¯»ç»å…¸è®ºæ–‡ã€‚"
            user_content = (
                f"ç»å…¸è®ºæ–‡ã€Š{title}ã€‹æ˜¯å¼•ç”¨é‡æé«˜çš„é•‡å±±ä¹‹ä½œã€‚\n"
                f"è¯·ä»¥ã€æŠ€æœ¯å²å­¦å®¶ã€‘çš„è§†è§’è§£è¯»ï¼š\n"
                f"1. **ğŸ’¥ ç ´å±€ç‚¹**ï¼šåœ¨å®ƒå‡ºç°ä¹‹å‰ï¼Œé¢†åŸŸé¢ä¸´ä»€ä¹ˆæ­»èƒ¡åŒï¼Ÿ\n"
                f"2. **ğŸ”‘ æ ¸å¿ƒé­”æ³•**ï¼šå®ƒç”¨ä»€ä¹ˆç®€å•çš„ç›´è§‰è§£å†³äº†é—®é¢˜ï¼Ÿ\n"
                f"3. **ğŸŒ ä¸–ç•Œå›å“**ï¼šå®ƒå¦‚ä½•å½±å“äº†åæ¥çš„ç ”ç©¶ï¼Ÿ\n"
                f"å­—æ•° 250 å­—ä»¥å†…ï¼Œä½¿ç”¨ Markdown åˆ—è¡¨æ ¼å¼ï¼Œä¿æŒç®€æ´ã€‚"
            )
        else:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¡ç®—æœºè§†è§‰ç ”ç©¶åŠ©æ‰‹ã€‚è¯·ç›´æ¥è¾“å‡ºåˆ†æå†…å®¹ï¼Œä¸è¦åŒ…å«'å¥½çš„'ã€'ä»¥ä¸‹æ˜¯åˆ†æ'ç­‰å®¢å¥—è¯ã€‚**å›ç­”å¿…é¡»éå¸¸ç®€æ˜æ‰¼è¦ï¼Œä¸¥æ ¼æ§åˆ¶ç¯‡å¹…ï¼Œåªåˆ—å‡ºæœ€æ ¸å¿ƒçš„å®šä¹‰ã€å…³é”®æŠ€æœ¯ç‚¹å’Œè¶‹åŠ¿ï¼Œé¿å…ä»»ä½•å†—ä½™è§£é‡Šã€‚**è¯·ä½¿ç”¨ Markdown æ ¼å¼ã€‚å¯¹äºæ•°å­¦å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ \\( ... \\) åŒ…è£¹ï¼Œç‹¬ç«‹å…¬å¼ç”¨ \\[ ... \\] åŒ…è£¹ã€‚"
            user_content = f"è¯·ç®€è¦åˆ†æè®¡ç®—æœºè§†è§‰ä¸­çš„æ¦‚å¿µ: {text}ã€‚ä¸Šä¸‹æ–‡ä¿¡æ¯: {json.dumps(context)}"

        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            "max_tokens": 1024
        }
        response = requests.post(url, headers=headers,
                                 json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        content = result['choices'][0]['message']['content']
        return {"summary": content, "keywords": ["DeepSeek-API"], "confidence": 1.0}
    except Exception as e:
        return {"error": f"DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}"}


def call_chatgpt(text, context, api_key):
    if not api_key:
        time.sleep(1)
        return {
            "summary": f"[ChatGPT æ¨¡å¼] (æœªé…ç½® API Key) ä½œä¸º AI è¯­è¨€æ¨¡å‹ï¼Œæˆ‘è®¤ä¸º '{text}' æ˜¯ä¸ªæœ‰è¶£çš„è¯é¢˜ã€‚",
            "keywords": ["OpenAI", "GPT-4", "NLP"],
            "confidence": 0.0
        }
    # å®é™…è°ƒç”¨ OpenAI API çš„ä»£ç ç¤ºä¾‹
    try:
        url = "https://api.openai.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}",
                   "Content-Type": "application/json"}

        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¡ç®—æœºè§†è§‰ç ”ç©¶åŠ©æ‰‹ã€‚è¯·ç›´æ¥è¾“å‡ºåˆ†æå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å®¢å¥—è¯ã€‚**å›ç­”å¿…é¡»éå¸¸ç®€æ˜æ‰¼è¦ï¼Œä¸¥æ ¼æ§åˆ¶ç¯‡å¹…ï¼Œåªåˆ—å‡ºæœ€æ ¸å¿ƒçš„å®šä¹‰ã€å…³é”®æŠ€æœ¯ç‚¹å’Œè¶‹åŠ¿ï¼Œé¿å…ä»»ä½•å†—ä½™è§£é‡Šã€‚**è¯·ä½¿ç”¨ Markdown æ ¼å¼ã€‚å¯¹äºæ•°å­¦å…¬å¼ï¼Œè¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ \\( ... \\) åŒ…è£¹ï¼Œç‹¬ç«‹å…¬å¼ç”¨ \\[ ... \\] åŒ…è£¹ã€‚"

        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"åˆ†æ CV è®ºæ–‡ä¸»é¢˜: {text}ã€‚èƒŒæ™¯: {context}"}
            ],
            "max_tokens": 1024
        }
        response = requests.post(url, headers=headers,
                                 json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        content = result['choices'][0]['message']['content']
        return {"summary": content, "keywords": ["GPT-API"], "confidence": 1.0}
    except Exception as e:
        return {"error": f"OpenAI API è°ƒç”¨å¤±è´¥: {str(e)}"}


def call_gemini(text, context, api_key):
    if not api_key:
        time.sleep(1)
        return {
            "summary": f"[Gemini æ¨¡å¼] (æœªé…ç½® API Key) Google çš„å¤šæ¨¡æ€æ¨¡å‹æ­£åœ¨åˆ†æ '{text}' çš„è§†è§‰ä¸æ–‡æœ¬å…³è”ã€‚",
            "keywords": ["Gemini", "Google", "Multimodal"],
            "confidence": 0.0
        }
    # TODO: å®ç° Gemini API è°ƒç”¨
    return {"summary": "Gemini API æš‚æœªå®ç°", "keywords": [], "confidence": 0.0}


def call_doubao(text, context, api_key):
    if not api_key:
        time.sleep(1)
        return {
            "summary": f"[è±†åŒ…æ¨¡å¼] (æœªé…ç½® API Key) å­—èŠ‚è·³åŠ¨è±†åŒ…å¤§æ¨¡å‹ä¸ºæ‚¨è§£è¯» '{text}'ã€‚",
            "keywords": ["Doubao", "ByteDance", "Chinese"],
            "confidence": 0.0
        }
    # TODO: å®ç° Doubao API è°ƒç”¨
    return {"summary": "Doubao API æš‚æœªå®ç°", "keywords": [], "confidence": 0.0}


@app.route('/api/analyze', methods=['POST'])
def analyze():
    try:
        data = request.json
        if not data:
            return jsonify({"error": "æ— æ•ˆçš„è¯·æ±‚æ•°æ®"}), 400

        text = data.get('text', '')
        context = data.get('context', {})
        model = data.get('model', 'mock')
        prompt_type = data.get('prompt_type')

        # ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„
        request_api_key = data.get('api_key', '')
        api_key = request_api_key if request_api_key else API_KEYS.get(
            model, "")

        print(
            f"[{time.strftime('%H:%M:%S')}] æ”¶åˆ°åˆ†æè¯·æ±‚: Model={model}, Type={prompt_type}, Text={text[:20]}...")

        if model == 'mock':
            time.sleep(0.8)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            result = generate_mock_response(text, context, prompt_type)
        elif model == 'deepseek':
            result = call_deepseek(text, context, api_key, prompt_type)
        elif model == 'chatgpt':
            result = call_chatgpt(text, context, api_key)
        elif model == 'gemini':
            result = call_gemini(text, context, api_key)
        elif model == 'doubao':
            result = call_doubao(text, context, api_key)
        else:
            return jsonify({"error": f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}"}), 400

        # å¦‚æœç»“æœä¸­æœ‰é”™è¯¯ä¿¡æ¯
        if "error" in result:
            return jsonify(result), 500

        return jsonify(result)

    except Exception as e:
        print(f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}")
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


@app.route('/api/health', methods=['GET'])
def health():
    return jsonify({
        "status": "ok",
        "models": list(API_KEYS.keys()) + ['mock'],
        "version": "1.0.0"
    })


if __name__ == '__main__':
    print("="*40)
    print("CV Explorer Backend Server Running")
    print("Address: http://localhost:5000")
    print("Supported Models: Mock, DeepSeek, ChatGPT, Gemini, Doubao")
    print("="*40)
    app.run(host='0.0.0.0', port=5000, debug=True)
