import pandas as pd
import numpy as np
import json
import re
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import umap
from collections import Counter, defaultdict
import itertools

# ================= è±ªåé…ç½®åŒºåŸŸ =================
INPUT_FILE = "../raw_data/cvpr_iccv_2015_2024_full.csv"
OUTPUT_JSON = "../visualization/data/final_data.json"
GPU_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# åœç”¨è¯è¡¨ (å»é™¤æ— æ„ä¹‰çš„å­¦æœ¯å¥—è¯)
STOP_WORDS = set([
    'learning', 'network', 'neural', 'deep', 'based', 'via', 'using', 'analysis', 
    'model', 'approach', 'method', 'algorithm', 'system', 'data', 'image', 'video',
    'object', 'detection', 'recognition', 'segmentation', 'visual', 'computer', 'vision',
    'cvpr', 'iccv', 'paper', 'proposed', 'state', 'art', 'performance', 'results',
    'towards', 'novel', 'framework', 'multi', 'super', 'resolution', 'robust', 'efficient'
])

# ç½‘ç»œå›¾é…ç½®ï¼šåªå±•ç¤º Top N ä¸ªæ ¸å¿ƒå¤§ä½¬ï¼Œå¦åˆ™å›¾ä¼šå¡æ­»
TOP_AUTHORS_LIMIT = 150 
# ===========================================

def clean_text(text):
    """åŸºç¡€æ¸…æ´—"""
    if not isinstance(text, str): return ""
    return re.sub(r'[^\w\s-]', '', text).lower()

def extract_keywords_simple(text):
    """ä»æ ‡é¢˜æå–å…³é”®è¯ (ç®€å•ç‰ˆï¼Œé€Ÿåº¦å¿«ä¸”æ•ˆæœå¥½)"""
    words = clean_text(text).split()
    return [w for w in words if w not in STOP_WORDS and len(w) > 3][:5]

def build_author_network(df):
    """æ„å»ºä½œè€…åˆä½œç½‘ç»œ (Nodes & Links)"""
    print("ğŸ•¸ï¸ æ­£åœ¨æ„å»ºä½œè€…å…³ç³»ç½‘ç»œ...")
    
    # 1. ç»Ÿè®¡ä½œè€…å‘æ–‡é‡
    author_counts = Counter()
    # å­˜å‚¨æ¯ç¯‡è®ºæ–‡çš„ä½œè€…åˆ—è¡¨
    paper_authors_list = []
    
    for authors_str in df['authors']:
        if not isinstance(authors_str, str): 
            paper_authors_list.append([])
            continue
        # åˆ†å‰²ä½œè€…å (æŒ‰é€—å·)
        names = [n.strip() for n in authors_str.split(',') if len(n.strip()) > 1]
        author_counts.update(names)
        paper_authors_list.append(names)
        
    # 2. ç­›é€‰ Top å¤§ä½¬ (ä¸ºäº†å¯è§†åŒ–æ€§èƒ½ï¼Œåªå–å‰ N å)
    top_authors = set([name for name, count in author_counts.most_common(TOP_AUTHORS_LIMIT)])
    
    # 3. æ„å»º Nodes
    nodes = []
    # è®°å½•æ¯ä¸ªä½œè€…çš„ ID (ç”¨äº d3 links source/target)
    for i, (name, count) in enumerate(author_counts.most_common(TOP_AUTHORS_LIMIT)):
        # group=1 æš‚æ—¶å ä½ï¼Œåé¢å¯ä»¥æ ¹æ®ç¤¾åŒºå‘ç°ç®—æ³•åˆ†ç»„
        node = {"id": name, "value": count, "group": 1} 
        nodes.append(node)
        
    # 4. æ„å»º Links (å…±ç°å…³ç³»)
    links_counter = Counter()
    
    for names in paper_authors_list:
        # åªä¿ç•™åœ¨ Top åˆ—è¡¨é‡Œçš„ä½œè€…
        valid_names = [n for n in names if n in top_authors]
        # å¦‚æœè¿™ç¯‡è®ºæ–‡æœ‰ä¸¤ä¸ªä»¥ä¸Šå¤§ä½¬åˆä½œï¼Œå»ºç«‹è¿æ¥
        if len(valid_names) > 1:
            # ç”Ÿæˆä¸¤ä¸¤ç»„åˆ (æ— å‘å›¾)
            for u, v in itertools.combinations(valid_names, 2):
                # æ’åºç¡®ä¿ A-B å’Œ B-A æ˜¯åŒä¸€æ¡è¾¹
                if u > v: u, v = v, u
                links_counter[(u, v)] += 1
                
    links = []
    for (u, v), weight in links_counter.items():
        links.append({"source": u, "target": v, "value": weight})
        
    return {"nodes": nodes, "links": links}

def process_data():
    print(f"ğŸš€ å¯åŠ¨å…¨æ ˆæ•°æ®å¤„ç† (Device: {GPU_DEVICE})...")
    
    # --- 1. æ•°æ®åŠ è½½ä¸æ¸…æ´— ---
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_FILE}")
        return

    df = pd.read_csv(INPUT_FILE)
    print(f"ğŸ“š åŸå§‹æ•°æ®: {len(df)} ç¯‡")
    
    # å‰”é™¤è„æ•°æ® (æ’¤ç¨¿å£°æ˜)
    df = df[~df['abstract'].str.contains("Violation of IEEE Publication Principles", na=False, case=False)]
    df.drop_duplicates(subset=['title'], inplace=True)
    df = df[df['abstract'].notna() & (df['abstract'] != "")]
    df['year'] = df['year'].astype(int)
    print(f"ğŸ§¹ æ¸…æ´—åæ•°æ®: {len(df)} ç¯‡")

    # --- 2. è¯­ä¹‰å‘é‡åŒ– (Embedding & UMAP) ---
    print("ğŸ§  è®¡ç®—è¯­ä¹‰å‘é‡ (Specter)...")
    embedder = SentenceTransformer('allenai-specter', device=GPU_DEVICE)
    text_corpus = (df['title'] + ' [SEP] ' + df['abstract']).tolist()
    
    # è¿™é‡Œ batch_size=32 æ¯”è¾ƒç¨³å¦¥ï¼Œ3090 å¯ä»¥å°è¯• 64
    embeddings = embedder.encode(text_corpus, convert_to_tensor=False, show_progress_bar=True, batch_size=32)
    
    print("ğŸ—ºï¸ é™ç»´ç”Ÿæˆåæ ‡ (UMAP)...")
    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', random_state=42)
    coords = reducer.fit_transform(embeddings)
    
    # å½’ä¸€åŒ–åæ ‡åˆ° [-1000, 1000]
    x_min, x_max = coords[:, 0].min(), coords[:, 0].max()
    y_min, y_max = coords[:, 1].min(), coords[:, 1].max()
    df['x'] = (coords[:, 0] - x_min) / (x_max - x_min) * 2000 - 1000
    df['y'] = (coords[:, 1] - y_min) / (y_max - y_min) * 2000 - 1000

    # --- 3. ç‰¹å¾æå– ---
    print("ğŸ·ï¸ æå–å…³é”®è¯...")
    df['keywords'] = df['title'].apply(extract_keywords_simple)

    # --- 4. æ„å»ºå¯è§†åŒ–æ•°æ®ç»“æ„ ---
    
    # Part A: æ•£ç‚¹å›¾æ•°æ® (Scatter Data)
    print("ğŸ“¦ æ‰“åŒ…æ•£ç‚¹å›¾æ•°æ®...")
    scatter_data = []
    # ç»Ÿè®¡æ¯å¹´çš„å…³é”®è¯åˆ†å¸ƒï¼Œä¸ºæ²³æµå›¾åšå‡†å¤‡
    year_keywords_raw = defaultdict(Counter)
    
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        kws = row['keywords']
        scatter_data.append({
            "id": idx,
            "title": row['title'],
            "year": row['year'],
            "conf": row['conf'],
            "authors": row['authors'],
            "x": round(row['x'], 2),
            "y": round(row['y'], 2),
            "kws": kws, 
        })
        # ç»Ÿè®¡å…³é”®è¯
        for w in kws:
            year_keywords_raw[row['year']][w] += 1

    # Part B: æ²³æµå›¾æ•°æ® (Streamgraph Data)
    print("ğŸŒŠ æ‰“åŒ…æ²³æµå›¾æ•°æ®...")
    # æ‰¾å‡ºåå¹´é—´æ€»é¢‘æ¬¡æœ€é«˜çš„ Top 20 å…³é”®è¯
    total_kw_counts = Counter()
    for y in year_keywords_raw:
        total_kw_counts.update(year_keywords_raw[y])
    
    # æ’é™¤ä¸€äº›ç‰¹åˆ«é€šç”¨çš„è¯
    exclude_stream = ['images', 'features', 'learning', 'networks']
    top_candidates = [k for k, v in total_kw_counts.most_common(50) if k not in exclude_stream]
    top_20_kws = top_candidates[:20]
    
    stream_data = []
    for year in sorted(year_keywords_raw.keys()):
        entry = {"year": year}
        for kw in top_20_kws:
            entry[kw] = year_keywords_raw[year][kw]
        stream_data.append(entry)

    # Part C: ä½œè€…ç½‘ç»œæ•°æ® (Network Data)
    network_data = build_author_network(df)

    # Part D: ç»Ÿè®¡é¢æ¿æ•°æ® (Statistics Data)
    print("ğŸ“Š æ‰“åŒ…ç»Ÿè®¡æ•°æ®...")
    stats_data = {
        "paper_counts": df.groupby('year').size().to_dict(), # æ¯å¹´å‘æ–‡é‡
        "conf_counts": df['conf'].value_counts().to_dict(),  # CVPR vs ICCV
        "top_keywords": top_20_kws
    }

    # --- 5. æœ€ç»ˆä¿å­˜ ---
    final_output = {
        "scatter": scatter_data,
        "stream": stream_data,
        "network": network_data,
        "stats": stats_data
    }
    
    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False) 
        
    print(f"\nâœ… å…¨æµç¨‹å®Œæˆï¼")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_JSON}")
    print(f"ğŸ‘‰ åŒ…å«æ¨¡å—: {list(final_output.keys())}")

if __name__ == "__main__":
    process_data()