import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import os
import re
from tqdm import tqdm
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ================= é…ç½®åŒºåŸŸ =================
OUTPUT_DIR = "../raw_data"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "cvpr_iccv_2015_2024_full.csv")
BASE_URL = "https://openaccess.thecvf.com"

TARGETS = [
    {"conf": "CVPR", "years": range(2015, 2025)},
    {"conf": "ICCV", "years": range(2015, 2024, 2)}
]

MAX_WORKERS = 4
# ===========================================

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)


def get_headers():
    return {
        "User-Agent": f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(100, 128)}.0.0.0 Safari/537.36"
    }


session = None


def get_session():
    global session
    if session is not None:
        return session
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=1,
                    status_forcelist=[500, 502, 503, 504, 429])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session


def fetch_page(url, retries=3):
    s = get_session()
    for i in range(retries):
        try:
            time.sleep(random.uniform(0.5, 1.5))
            response = s.get(url, headers=get_headers(), timeout=30)
            if response.status_code == 200:
                return BeautifulSoup(response.text, 'html.parser')
            elif response.status_code == 429:
                print(f"âš ï¸ è§¦å‘é™æµï¼Œæš‚åœ 30 ç§’...")
                time.sleep(30)
        except Exception as e:
            if "SSLEOFError" in str(e):
                time.sleep(5)
            elif i == retries - 1:
                print(f"âŒ è¯·æ±‚å½»åº•å¤±è´¥: {url} | {e}")
    return None

# ================= æ ¸å¿ƒä¿®å¤ï¼šå…¨èƒ½å‹ä½œè€…æå–é€»è¾‘ =================


def parse_paper_list(soup, year, conf):
    papers = []
    titles = soup.find_all('dt', class_='ptitle')

    # è°ƒè¯•è®¡æ•°å™¨
    debug_count = 0

    for dt in titles:
        # 1. æå–é“¾æ¥
        a_tag = dt.find('a')
        link = urljoin(BASE_URL, a_tag['href']) if a_tag else ""
        title_text = dt.text.strip()

        # 2. æå–ä½œè€… (é’ˆå¯¹ 2015-2025 å…¨å¹´ä»½é€‚é…)
        authors = ""
        dd = dt.find_next_sibling('dd')

        if dd:
            auth_parts = []

            # ç­–ç•¥ A: ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ <form> é‡Œçš„ <a> æ ‡ç­¾ (é’ˆå¯¹ 2025 æ–°ç‰ˆç»“æ„)
            # ç»“æ„: <form ...><a ...>Author Name</a></form>
            forms = dd.find_all('form')
            if forms:
                for form in forms:
                    # æå– form ä¸‹çš„æ‰€æœ‰ a æ ‡ç­¾æ–‡æœ¬
                    for a in form.find_all('a'):
                        text = a.text.strip()
                        if text and text.lower() != "bibtex":  # æ’é™¤ bibtex æŒ‰é’®
                            auth_parts.append(text)

            # ç­–ç•¥ B: å¦‚æœæ²¡æ‰¾åˆ° form é‡Œçš„ä½œè€…ï¼Œæˆ–è€…æ‰¾å®Œäº† form è¿˜è¦æ‰¾è£¸éœ²çš„æ–‡æœ¬ (é’ˆå¯¹ 2015 æ—§ç‰ˆç»“æ„)
            # éå† dd çš„ç›´æ¥å­èŠ‚ç‚¹
            # æ³¨æ„ï¼šæ–°ç‰ˆç»“æ„é‡Œä½œè€…éƒ½åœ¨ form é‡Œï¼Œæ—§ç‰ˆåœ¨è£¸æ–‡æœ¬é‡Œï¼Œä¸¤è€…æ··åˆå¤„ç†
            if not auth_parts:  # åªæœ‰å½“ç­–ç•¥Aæ²¡æ‰¾åˆ°æ—¶ï¼Œæ‰å¯ç”¨ç­–ç•¥Bï¼Œé¿å…é‡å¤æˆ–æ··ä¹±
                for content in dd.contents:
                    # å¿½ç•¥ Tag ç±»å‹çš„ form (å› ä¸ºä¸Šé¢ç­–ç•¥Aå¤„ç†è¿‡äº†) å’Œ div
                    if content.name in ['form', 'div']:
                        continue

                    # æå–çº¯æ–‡æœ¬ (2015å¹´æ ·å¼)
                    if isinstance(content, str):
                        text = content.strip()
                        # æ’é™¤æ‰åªæœ‰é€—å·æˆ–ç©ºå­—ç¬¦çš„æƒ…å†µ
                        if text and text != ',':
                            auth_parts.append(text)

                    # æå–ç›´æ¥é“¾æ¥ (2016-2024 ä¸­é—´å¹´ä»½æ ·å¼)
                    elif content.name == 'a':
                        text = content.text.strip()
                        if text:
                            auth_parts.append(text)

            # æ‹¼æ¥ç»“æœ
            full_text = ", ".join(auth_parts)
            # ç»ˆææ¸…æ´—:
            # 1. æŠŠ ", ," æ›¿æ¢æˆ ","
            # 2. å»æ‰é¦–å°¾é€—å·
            cleaned = re.sub(r'\s*,\s*', ', ', full_text)
            authors = cleaned.strip().strip(',')

            # --- è°ƒè¯•æ‰“å° (ä»…é’ˆå¯¹ç¬¬ä¸€ç¯‡) ---
            if debug_count == 0:
                print(f"\nğŸ” [DEBUG {year}] è§£æç¤ºä¾‹: {title_text[:30]}...")
                print(f"   -> æå–åˆ°çš„ä½œè€…: [{authors}]")
            debug_count += 1
            # --------------------------------------

        papers.append({
            "conf": conf,
            "year": year,
            "title": title_text,
            "authors": authors,
            "link": link,
            "abstract": ""
        })
    return papers

# ================= å‰©ä½™éƒ¨åˆ†ä¿æŒä¸å˜ =================


def crawl_list_phase():
    all_papers = []
    print("ğŸš€ [é˜¶æ®µä¸€] å¼€å§‹æŠ“å–è®ºæ–‡åˆ—è¡¨ç›®å½•...")

    for target in TARGETS:
        conf = target['conf']
        for year in target['years']:
            print(f"   æ­£åœ¨æ‰«æ {conf} {year} ...")
            url_all = f"{BASE_URL}/{conf}{year}?day=all"
            soup = fetch_page(url_all)
            papers = []

            if soup:
                papers = parse_paper_list(soup, year, conf)

            if not papers:
                main_url = f"{BASE_URL}/{conf}{year}"
                soup = fetch_page(main_url)
                if soup:
                    day_links = set()
                    for a in soup.find_all('a', href=True):
                        if 'day=' in a['href']:
                            day_links.add(urljoin(BASE_URL, a['href']))
                    if day_links:
                        print(f"     -> æ£€æµ‹åˆ° {len(day_links)} ä¸ªå­é¡µé¢...")
                        for day_url in day_links:
                            day_soup = fetch_page(day_url)
                            if day_soup:
                                papers.extend(parse_paper_list(
                                    day_soup, year, conf))
                    else:
                        papers = parse_paper_list(soup, year, conf)

            if papers:
                print(f"     âœ… è·å– {len(papers)} ç¯‡")
                all_papers.extend(papers)
            else:
                print(f"     âŒ æœªæ‰¾åˆ°æ•°æ®")

    return pd.DataFrame(all_papers)


def fetch_abstract_worker(link):
    if not link:
        return None
    soup = fetch_page(link)
    if not soup:
        return None
    abs_div = soup.find('div', id='abstract')
    if abs_div:
        return abs_div.text.strip()
    meta = soup.find('meta', attrs={'name': 'citation_abstract'})
    if meta:
        return meta['content'].strip()
    return None


def crawl_detail_phase(df):
    print(f"\nğŸš€ [é˜¶æ®µäºŒ] å¼€å§‹è¡¥å…¨æ‘˜è¦ (å…± {len(df)} ç¯‡)...")
    if 'abstract' not in df.columns:
        df['abstract'] = ""
    todo_indices = df[df['abstract'].isna() | (
        df['abstract'] == "")].index.tolist()
    print(f"   ğŸ“‹ å¾…æŠ“å–æ•°é‡: {len(todo_indices)} (å·²è·³è¿‡ç°æœ‰æ•°æ®)")
    if not todo_indices:
        return df

    save_counter = 0
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_idx = {executor.submit(
            fetch_abstract_worker, df.loc[idx, 'link']): idx for idx in todo_indices}
        for future in tqdm(as_completed(future_to_idx), total=len(todo_indices), desc="Downloading Abstracts"):
            idx = future_to_idx[future]
            try:
                abstract = future.result()
                if abstract:
                    df.at[idx, 'abstract'] = abstract
            except Exception:
                pass
            save_counter += 1
            if save_counter >= 100:
                df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
                save_counter = 0
    return df


def main():
    # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦é‡è·‘åˆ—è¡¨
    run_list_phase = True
    if os.path.exists(OUTPUT_FILE):
        try:
            df = pd.read_csv(OUTPUT_FILE)
            if len(df) > 0 and pd.isna(df.iloc[0]['authors']):
                print("âš ï¸ æ£€æµ‹åˆ°ä½œè€…ä¿¡æ¯ä¸ºç©ºï¼Œæ­£åœ¨é‡è·‘åˆ—è¡¨æŠ“å–...")
                run_list_phase = True
            elif len(df) > 100:
                run_list_phase = False
                print("âœ… ç°æœ‰æ•°æ®æ­£å¸¸ï¼Œè¿›å…¥æ‘˜è¦è¡¥å…¨ã€‚")
        except:
            run_list_phase = True

    if run_list_phase:
        if os.path.exists(OUTPUT_FILE):
            os.remove(OUTPUT_FILE)
        df = crawl_list_phase()
        df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    else:
        df = pd.read_csv(OUTPUT_FILE)

    df = crawl_detail_phase(df)

    print("\nğŸ§¹ æœ€ç»ˆæ¸…æ´—...")
    df.drop_duplicates(subset=['title'], inplace=True)
    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    print(f"ğŸ‰ å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
