# github_crawler_smart.py
import os
import requests
import time
import csv
import base64
import math
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# ==================== æ™ºèƒ½é…ç½®ç±» ====================
class SmartConfig:
    """æ™ºèƒ½é…ç½®ç®¡ç†"""
    # GitHubä»¤ç‰Œ
    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
    if not GITHUB_TOKEN:
        raise ValueError("âŒ é”™è¯¯ï¼šè¯·è®¾ç½® GITHUB_TOKEN ç¯å¢ƒå˜é‡\n   å‘½ä»¤: set GITHUB_TOKEN=ä½ çš„ä»¤ç‰Œ")

    # çˆ¬å–ç›®æ ‡
    TARGET_REPOS = 500  # ç›®æ ‡ä»“åº“æ€»æ•°
    README_SAMPLE = 500  # è·å–READMEçš„æ ·æœ¬æ•°ï¼ˆå‡å°‘ä»¥é™ä½APIå‹åŠ›ï¼‰
    DEEP_ANALYSIS = 500  # æ·±åº¦åˆ†æçš„ä»“åº“æ•°

    # æ™ºèƒ½å»¶è¿Ÿç­–ç•¥
    MIN_DELAY = 3.0  # æœ€å°è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
    MAX_DELAY = 15.0  # æœ€å¤§è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
    BASE_DELAY = 3.5  # åŸºç¡€å»¶è¿Ÿ
    DELAY_INCREMENT = 1.3  # å»¶è¿Ÿé€’å¢å› å­
    BATCH_EXTRA_DELAY = 8.0  # æ‰¹æ¬¡é—´é¢å¤–å»¶è¿Ÿ

    # æ‰¹é‡å¤„ç†
    BATCH_SIZE = 8  # å‡å°æ‰¹é‡å¤§å°
    SEARCH_BATCH_SIZE = 2  # æœç´¢APIæ‰¹é‡æ›´å°

    # é€€é¿ç­–ç•¥
    MAX_RETRIES = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
    RETRY_BACKOFF = 2.0  # é‡è¯•é€€é¿å› å­

    # è¾“å‡º
    OUTPUT_FILE = f"github_top_{TARGET_REPOS}_smart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"


config = SmartConfig()


# ==================== æ™ºèƒ½APIç®¡ç†å™¨ ====================
class SmartAPIManager:
    """æ™ºèƒ½APIç®¡ç†å™¨ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥"""

    def __init__(self):
        self.headers = {
            'Authorization': f'token {config.GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json'
        }
        self.search_api_used = 0
        self.core_api_used = 0
        self.last_request_time = 0
        self.current_delay = config.BASE_DELAY
        self.consecutive_failures = 0
        self.last_reset_check = 0

    def _calculate_dynamic_delay(self) -> float:
        """è®¡ç®—åŠ¨æ€å»¶è¿Ÿ"""
        # åŸºç¡€å»¶è¿Ÿ
        delay = self.current_delay

        # æ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°å¢åŠ å»¶è¿Ÿ
        if self.consecutive_failures > 0:
            delay *= (1 + self.consecutive_failures * 0.5)

        # ç¡®ä¿åœ¨æœ€å°æœ€å¤§èŒƒå›´å†…
        return max(config.MIN_DELAY, min(delay, config.MAX_DELAY))

    def _update_delay_based_on_response(self, response, api_type: str):
        """æ ¹æ®å“åº”æ›´æ–°å»¶è¿Ÿç­–ç•¥"""
        if response is None:
            self.consecutive_failures += 1
            self.current_delay = min(
                config.MAX_DELAY,
                self.current_delay * config.DELAY_INCREMENT
            )
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œå¢åŠ å»¶è¿Ÿè‡³ {self.current_delay:.1f}ç§’")
            return

        # è¯·æ±‚æˆåŠŸï¼Œå‡å°‘è¿ç»­å¤±è´¥è®¡æ•°
        if self.consecutive_failures > 0:
            self.consecutive_failures = max(0, self.consecutive_failures - 1)

        # æ£€æŸ¥APIé™é¢
        if hasattr(response, 'headers'):
            remaining = response.headers.get('X-RateLimit-Remaining')
            if remaining:
                remaining_int = int(remaining)

                # æ ¹æ®å‰©ä½™é™é¢è°ƒæ•´å»¶è¿Ÿ
                if remaining_int < 100:
                    # é™é¢ç´§å¼ ï¼Œå¢åŠ å»¶è¿Ÿ
                    self.current_delay = min(
                        config.MAX_DELAY,
                        self.current_delay * 1.2
                    )
                elif remaining_int > 1000 and self.current_delay > config.BASE_DELAY:
                    # é™é¢å……è¶³ï¼Œé€‚å½“å‡å°‘å»¶è¿Ÿ
                    self.current_delay = max(
                        config.MIN_DELAY,
                        self.current_delay * 0.9
                    )

        # æ£€æŸ¥æ˜¯å¦è§¦å‘æ¬¡è¦é™åˆ¶
        if hasattr(response, 'status_code') and response.status_code == 403:
            if 'secondary' in response.text.lower() or 'rate limit' in response.text.lower():
                print("ğŸ”´ æ£€æµ‹åˆ°æ¬¡è¦é¢‘ç‡é™åˆ¶ï¼Œå¤§å¹…å¢åŠ å»¶è¿Ÿ")
                self.current_delay = min(
                    config.MAX_DELAY,
                    self.current_delay * 2.0
                )
                self.consecutive_failures += 2

    def _wait_for_rate_limit_reset(self, reset_timestamp: int) -> bool:
        """ç­‰å¾…APIé™é¢é‡ç½®"""
        now = int(time.time())
        wait_seconds = reset_timestamp - now + 2

        if wait_seconds > 0:
            print(f"â³ APIé™åˆ¶ï¼Œç­‰å¾… {wait_seconds} ç§’ ({wait_seconds // 60}åˆ†{wait_seconds % 60}ç§’)...")

            # æ˜¾ç¤ºå€’è®¡æ—¶
            for remaining in range(wait_seconds, 0, -60):
                if remaining > 60:
                    print(f"   å‰©ä½™çº¦ {remaining // 60} åˆ†é’Ÿ...")
                    time.sleep(60)
                else:
                    time.sleep(remaining)
                    break

            print("âœ… APIé™é¢å·²é‡ç½®ï¼Œç»§ç»­æ‰§è¡Œ")
            return True
        return False

    def make_smart_request(self, url: str, api_type: str = 'core') -> Optional[Dict]:
        """æ™ºèƒ½è¯·æ±‚ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥"""

        # APIä½¿ç”¨ç»Ÿè®¡
        if api_type == 'search':
            if self.search_api_used >= 30:  # GitHubæœç´¢APIç¡¬é™åˆ¶
                print(f"âš ï¸ æœç´¢APIå·²è¾¾é™é¢ ({self.search_api_used}/30)")
                return None
        else:
            if self.core_api_used >= 5000:  # GitHubæ ¸å¿ƒAPIç¡¬é™åˆ¶
                print(f"âš ï¸ æ ¸å¿ƒAPIå·²è¾¾é™é¢ ({self.core_api_used}/5000)")
                return None

        # æŒ‡æ•°é€€é¿é‡è¯•
        for attempt in range(config.MAX_RETRIES):
            try:
                # åŠ¨æ€å»¶è¿Ÿæ§åˆ¶
                current_delay = self._calculate_dynamic_delay()
                time_since_last = time.time() - self.last_request_time

                if time_since_last < current_delay:
                    sleep_time = current_delay - time_since_last
                    if sleep_time > 0.1:
                        time.sleep(sleep_time)

                # å‘é€è¯·æ±‚
                print(
                    f"  ğŸ“¤ è¯·æ±‚ {api_type.upper()} API (å°è¯• {attempt + 1}/{config.MAX_RETRIES}, å»¶è¿Ÿ {current_delay:.1f}s)")
                response = requests.get(
                    url,
                    headers=self.headers,
                    verify=False,
                    timeout=45  # æ›´é•¿è¶…æ—¶
                )

                self.last_request_time = time.time()

                # æ›´æ–°APIä½¿ç”¨è®¡æ•°
                if api_type == 'search':
                    self.search_api_used += 1
                else:
                    self.core_api_used += 1

                # å¤„ç†å“åº”
                if response.status_code == 200:
                    # è¯·æ±‚æˆåŠŸï¼Œæ›´æ–°å»¶è¿Ÿç­–ç•¥
                    self._update_delay_based_on_response(response, api_type)

                    # æ˜¾ç¤ºAPIçŠ¶æ€
                    if attempt > 0:
                        print(f"  âœ… è¯·æ±‚æˆåŠŸ (ç¬¬{attempt + 1}æ¬¡å°è¯•)")

                    remaining = response.headers.get('X-RateLimit-Remaining', 'æœªçŸ¥')
                    limit = response.headers.get('X-RateLimit-Limit', 'æœªçŸ¥')

                    if api_type == 'search' and int(remaining) if remaining.isdigit() else 100 < 10:
                        print(f"  âš ï¸  æœç´¢APIä»…å‰© {remaining}/{limit} æ¬¡")

                    return response.json()

                elif response.status_code == 403:
                    # APIé™åˆ¶å¤„ç†
                    reset_time = response.headers.get('X-RateLimit-Reset')

                    if reset_time and 'rate limit' in response.text.lower():
                        reset_timestamp = int(reset_time)

                        # å¦‚æœæ˜¯æ¬¡è¦é™åˆ¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                        if 'secondary' in response.text.lower():
                            print("ğŸ”´ è§¦å‘æ¬¡è¦é¢‘ç‡é™åˆ¶")
                            wait_time = 300  # æ¬¡è¦é™åˆ¶ç­‰å¾…5åˆ†é’Ÿ
                            print(f"â³ æ¬¡è¦é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                            time.sleep(wait_time)
                            continue

                        # ä¸»è¦é™åˆ¶ï¼Œç­‰å¾…åˆ°é‡ç½®æ—¶é—´
                        if self._wait_for_rate_limit_reset(reset_timestamp):
                            continue

                    # å…¶ä»–403é”™è¯¯
                    print(f"  ğŸ”’ 403é”™è¯¯: {response.text[:150]}")
                    self._update_delay_based_on_response(response, api_type)

                elif response.status_code == 429:
                    # å¤ªå¤šè¯·æ±‚
                    print("  ğŸš« 429: è¯·æ±‚è¿‡å¤š")
                    self.current_delay = min(config.MAX_DELAY, self.current_delay * 1.8)
                    retry_after = response.headers.get('Retry-After', 60)
                    time.sleep(int(retry_after))
                    continue

                else:
                    # å…¶ä»–HTTPé”™è¯¯
                    print(f"  âŒ HTTP {response.status_code}: {response.text[:100]}")
                    self._update_delay_based_on_response(response, api_type)

                # æŒ‡æ•°é€€é¿ç­‰å¾…
                wait_time = config.RETRY_BACKOFF ** attempt
                print(f"  â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)

            except requests.exceptions.Timeout:
                print(f"  â±ï¸  è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{config.MAX_RETRIES})")
                self.consecutive_failures += 1
                self.current_delay = min(config.MAX_DELAY, self.current_delay * 1.3)
                time.sleep(config.RETRY_BACKOFF ** attempt)

            except requests.exceptions.ConnectionError:
                print(f"  ğŸ”Œ è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{config.MAX_RETRIES})")
                self.consecutive_failures += 1
                self.current_delay = min(config.MAX_DELAY, self.current_delay * 1.5)
                time.sleep(config.RETRY_BACKOFF ** attempt * 2)

            except Exception as e:
                print(f"  âš ï¸  å¼‚å¸¸: {type(e).__name__}: {str(e)[:100]}")
                self.consecutive_failures += 1
                time.sleep(config.RETRY_BACKOFF ** attempt)

        print(f"  âŒ è¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {config.MAX_RETRIES} æ¬¡: {url}")
        self.consecutive_failures += 1
        self.current_delay = min(config.MAX_DELAY, self.current_delay * 1.5)
        return None

    def get_api_status(self) -> Dict:
        """è·å–å½“å‰APIçŠ¶æ€"""
        status = {
            'search_used': self.search_api_used,
            'core_used': self.core_api_used,
            'current_delay': self.current_delay,
            'consecutive_failures': self.consecutive_failures
        }

        # æ£€æŸ¥å®é™…é™é¢
        try:
            url = "https://api.github.com/rate_limit"
            data = self.make_smart_request(url, 'core')
            if data:
                status['search_remaining'] = data['resources']['search']['remaining']
                status['core_remaining'] = data['resources']['core']['remaining']
                status['search_limit'] = data['resources']['search']['limit']
                status['core_limit'] = data['resources']['core']['limit']
        except:
            pass

        return status


# ==================== æ™ºèƒ½çˆ¬è™«ç±» ====================
class SmartGitHubCrawler:
    """æ™ºèƒ½GitHubä»“åº“çˆ¬è™«"""

    def __init__(self):
        self.api = SmartAPIManager()
        self.repos = []
        self.start_time = time.time()

    def crawl_intelligently(self) -> List[Dict]:
        """æ™ºèƒ½çˆ¬å–æ‰€æœ‰æ•°æ®"""
        print("=" * 70)
        print("ğŸš€ æ™ºèƒ½GitHubä»“åº“çˆ¬è™«å¯åŠ¨")
        print(f"ğŸ“Š ç›®æ ‡: {config.TARGET_REPOS}ä¸ªä»“åº“")
        print(f"âš™ï¸  é…ç½®: å»¶è¿Ÿ {config.MIN_DELAY}-{config.MAX_DELAY}ç§’, æ‰¹é‡ {config.BATCH_SIZE}")
        print("=" * 70)

        # æ˜¾ç¤ºåˆå§‹APIçŠ¶æ€
        self._print_api_status()

        try:
            # æ­¥éª¤1: è·å–ä»“åº“åŸºç¡€ä¿¡æ¯
            print("\n" + "=" * 50)
            print("ğŸ“Š æ­¥éª¤1: è·å–ä»“åº“åŸºç¡€ä¿¡æ¯")
            print("=" * 50)

            basic_repos = self._get_basic_repositories()
            if not basic_repos:
                print("âŒ æœªè·å–åˆ°åŸºç¡€ä¿¡æ¯")
                return []

            # æ­¥éª¤2: æ™ºèƒ½è¡¥å……è¯¦ç»†ä¿¡æ¯
            print("\n" + "=" * 50)
            print("ğŸ“ˆ æ­¥éª¤2: æ™ºèƒ½è¡¥å……è¯¦ç»†ä¿¡æ¯")
            print("=" * 50)

            detailed_repos = self._enrich_repositories(basic_repos)

            # æ­¥éª¤3: æ·±åº¦åˆ†æï¼ˆå¯é€‰ï¼‰
            if config.DEEP_ANALYSIS > 0:
                print("\n" + "=" * 50)
                print("ğŸ” æ­¥éª¤3: æ·±åº¦åˆ†æ")
                print("=" * 50)

                final_repos = self._deep_analyze(detailed_repos)
            else:
                final_repos = detailed_repos

            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - self.start_time
            print(f"\nâœ… æ‰€æœ‰æ­¥éª¤å®Œæˆ!")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time / 60:.1f}åˆ†é’Ÿ)")

            return final_repos

        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²è·å–æ•°æ®...")
            return self.repos
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {type(e).__name__}: {e}")
            return self.repos

    def _get_basic_repositories(self) -> List[Dict]:
        """è·å–åŸºç¡€ä»“åº“ä¿¡æ¯"""
        all_repos = []
        pages_needed = math.ceil(config.TARGET_REPOS / 100)

        print(f"éœ€è¦è·å– {pages_needed} é¡µæ•°æ® (æ¯é¡µ100ä¸ª)")

        for page in range(1, pages_needed + 1):
            print(f"\nğŸ“„ è·å–ç¬¬ {page}/{pages_needed} é¡µ...")

            # æ‰¹æ¬¡é—´é¢å¤–å»¶è¿Ÿ
            if page > 1:
                extra_delay = config.BATCH_EXTRA_DELAY
                print(f"â³ é¡µé—´å»¶è¿Ÿ {extra_delay}ç§’...")
                time.sleep(extra_delay)

            url = (
                f"https://api.github.com/search/repositories?"
                f"q=stars:>1000&sort=stars&order=desc&per_page=100&page={page}"
            )

            data = self.api.make_smart_request(url, api_type='search')

            if data and 'items' in data:
                for item in data['items']:
                    if len(all_repos) >= config.TARGET_REPOS:
                        break

                    repo_info = {
                        'id': item['id'],
                        'full_name': item['full_name'],
                        'url': item['html_url'],
                        'description': (item.get('description') or '')[:200],
                        'stars': item['stargazers_count'],
                        'forks': item.get('forks_count', 0),
                        'language': item.get('language', '') or 'æœªçŸ¥',
                        'created_at': item.get('created_at', '')[:10],
                        'updated_at': item.get('pushed_at', '')[:10],
                        'open_issues': item.get('open_issues_count', 0),
                        'topics': ', '.join(item.get('topics', [])[:3]),
                        'license': (item.get('license', {}) or {}).get('name', 'æ— '),
                        'readme_summary': 'å¾…è·å–',
                        'has_readme': False
                    }
                    all_repos.append(repo_info)

            print(f"  âœ… å·²è·å–: {len(all_repos)}/{config.TARGET_REPOS}")

            # æ˜¾ç¤ºå½“å‰APIçŠ¶æ€
            if page % 2 == 0:
                self._print_api_status()

        print(f"\nğŸ¯ åŸºç¡€ä¿¡æ¯è·å–å®Œæˆ: {len(all_repos)} ä¸ªä»“åº“")
        return all_repos

    def _enrich_repositories(self, repos: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½è¡¥å……ä»“åº“è¯¦ç»†ä¿¡æ¯"""
        print(f"å‡†å¤‡è¡¥å…… {min(config.README_SAMPLE, len(repos))} ä¸ªä»“åº“çš„è¯¦ç»†ä¿¡æ¯")

        for i in range(0, min(config.README_SAMPLE, len(repos)), config.BATCH_SIZE):
            batch_end = min(i + config.BATCH_SIZE, config.README_SAMPLE, len(repos))
            batch = repos[i:batch_end]

            print(f"\nğŸ”§ å¤„ç†æ‰¹æ¬¡ {i + 1}-{batch_end}/{min(config.README_SAMPLE, len(repos))}")

            for j, repo in enumerate(batch):
                repo_idx = i + j + 1

                # è·å–README
                if repo_idx <= config.README_SAMPLE:
                    readme = self._get_readme_intelligent(repo['full_name'])
                    repo['readme_summary'] = readme
                    repo['has_readme'] = readme != "æ— README" and readme != "è·å–å¤±è´¥"

                # æ›´æ–°è¿›åº¦
                if (repo_idx) % 5 == 0:
                    print(f"  è¿›åº¦: {repo_idx}/{min(config.README_SAMPLE, len(repos))}")
                    self._print_progress_bar(repo_idx, min(config.README_SAMPLE, len(repos)))

            # æ‰¹æ¬¡é—´æ™ºèƒ½å»¶è¿Ÿ
            if batch_end < min(config.README_SAMPLE, len(repos)):
                batch_delay = config.BATCH_EXTRA_DELAY * (1 + self.api.consecutive_failures * 0.3)
                print(f"â³ æ‰¹æ¬¡é—´å»¶è¿Ÿ {batch_delay:.1f}ç§’...")
                time.sleep(batch_delay)

        print(f"\nâœ… è¯¦ç»†ä¿¡æ¯è¡¥å……å®Œæˆ")
        return repos

    def _deep_analyze(self, repos: List[Dict]) -> List[Dict]:
        """æ·±åº¦åˆ†æå‰Nä¸ªä»“åº“"""
        analysis_count = min(config.DEEP_ANALYSIS, len(repos))

        if analysis_count <= 0:
            return repos

        print(f"æ·±åº¦åˆ†æå‰ {analysis_count} ä¸ªé«˜æ˜Ÿä»“åº“")

        for i, repo in enumerate(repos[:analysis_count]):
            print(f"\nğŸ” åˆ†æ {i + 1}/{analysis_count}: {repo['full_name']}")

            # è·å–è´¡çŒ®è€…ä¿¡æ¯
            contributors = self._get_top_contributors(repo['full_name'])
            if contributors:
                repo['top_contributor'] = contributors[0].get('login', '')
                repo['contributor_count'] = len(contributors)

            # è·å–æœ€è¿‘æäº¤ä¿¡æ¯
            commits = self._get_recent_commits(repo['full_name'])
            if commits:
                repo['recent_commits'] = len(commits)
                if commits:
                    last_commit = commits[0].get('commit', {}).get('author', {}).get('date', '')
                    if last_commit:
                        repo['last_commit_date'] = last_commit[:10]

            # è®¡ç®—æ´»è·ƒåº¦åˆ†æ•°
            repo['activity_score'] = self._calculate_activity_score(repo)

            # æ¯5ä¸ªä»“åº“æ˜¾ç¤ºä¸€æ¬¡APIçŠ¶æ€
            if (i + 1) % 5 == 0:
                self._print_api_status()

        print(f"\nâœ… æ·±åº¦åˆ†æå®Œæˆ: {analysis_count} ä¸ªä»“åº“")
        return repos

    def _get_readme_intelligent(self, full_name: str) -> str:
        """æ™ºèƒ½è·å–READMEå†…å®¹"""
        owner, repo_name = full_name.split('/', 1)
        url = f"https://api.github.com/repos/{owner}/{repo_name}/readme"

        data = self.api.make_smart_request(url, api_type='core')

        if data and 'content' in data:
            try:
                content = base64.b64decode(data['content']).decode('utf-8', errors='ignore')
                # æ™ºèƒ½æå–æ‘˜è¦
                lines = content.split('\n')
                summary_lines = []

                # æå–å‰5ä¸ªéç©ºè¡Œæˆ–æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        summary_lines.append(line)
                        if len(summary_lines) >= 3:
                            break

                if summary_lines:
                    summary = ' '.join(summary_lines)[:400]
                else:
                    # å¦‚æœæ²¡æœ‰åˆé€‚å†…å®¹ï¼Œå–å‰200ä¸ªå­—ç¬¦
                    summary = content[:200]

                return summary + "..." if len(content) > len(summary) else summary

            except Exception as e:
                print(f"  âš ï¸ READMEè§£ç å¤±è´¥: {e}")
                return "è§£ç å¤±è´¥"

        return "æ— README"

    def _get_top_contributors(self, full_name: str, limit: int = 3) -> List[Dict]:
        """è·å–å‰å‡ ä½è´¡çŒ®è€…"""
        url = f"https://api.github.com/repos/{full_name}/contributors?per_page={limit}"
        return self.api.make_smart_request(url, api_type='core') or []

    def _get_recent_commits(self, full_name: str, limit: int = 5) -> List[Dict]:
        """è·å–æœ€è¿‘æäº¤"""
        url = f"https://api.github.com/repos/{full_name}/commits?per_page={limit}"
        return self.api.make_smart_request(url, api_type='core') or []

    def _calculate_activity_score(self, repo: Dict) -> float:
        """è®¡ç®—æ´»è·ƒåº¦åˆ†æ•°"""
        score = 50.0  # åŸºç¡€åˆ†

        try:
            # åŸºäºæ›´æ–°æ—¶é—´çš„åˆ†æ•°
            if 'updated_at' in repo and repo['updated_at']:
                last_update = datetime.strptime(repo['updated_at'], '%Y-%m-%d')
                days_since = (datetime.now() - last_update).days

                if days_since < 7:
                    score += 25
                elif days_since < 30:
                    score += 15
                elif days_since < 90:
                    score += 5
                elif days_since > 365:
                    score -= 15

            # åŸºäºstaræ•°é‡çš„åˆ†æ•°
            stars = repo.get('stars', 0)
            if stars > 50000:
                score += 15
            elif stars > 10000:
                score += 10
            elif stars > 1000:
                score += 5

            # åŸºäºIssueæ´»è·ƒåº¦çš„åˆ†æ•°
            open_issues = repo.get('open_issues', 0)
            if stars > 0 and open_issues > 0:
                issue_ratio = open_issues / stars
                if issue_ratio < 0.01:
                    score += 10  # Issueæ¯”ä¾‹ä½ï¼Œç»´æŠ¤è‰¯å¥½
                elif issue_ratio > 0.1:
                    score -= 5  # Issueæ¯”ä¾‹é«˜ï¼Œå¯èƒ½æœ‰é—®é¢˜

        except Exception as e:
            print(f"  âš ï¸ æ´»è·ƒåº¦è®¡ç®—å¤±è´¥: {e}")

        return round(max(0, min(score, 100)), 1)

    def _print_api_status(self):
        """æ‰“å°å½“å‰APIçŠ¶æ€"""
        status = self.api.get_api_status()

        print("\nğŸ“Š å½“å‰APIçŠ¶æ€:")
        print(f"  æœç´¢API: {status.get('search_used', 0)}/{status.get('search_limit', 30)}æ¬¡")
        print(f"  æ ¸å¿ƒAPI: {status.get('core_used', 0)}/{status.get('core_limit', 5000)}æ¬¡")
        print(f"  å½“å‰å»¶è¿Ÿ: {status.get('current_delay', config.BASE_DELAY):.1f}ç§’")
        print(f"  è¿ç»­å¤±è´¥: {status.get('consecutive_failures', 0)}æ¬¡")

    def _print_progress_bar(self, current: int, total: int, length: int = 30):
        """æ‰“å°è¿›åº¦æ¡"""
        percent = current / total
        filled_length = int(length * percent)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (length - filled_length)
        print(f"  [{bar}] {current}/{total} ({percent:.1%})")


# ==================== æ•°æ®ä¿å­˜ ====================
def save_repositories_to_csv(repos: List[Dict], filename: str):
    """ä¿å­˜ä»“åº“æ•°æ®åˆ°CSV"""
    if not repos:
        print("âŒ æ— æ•°æ®å¯ä¿å­˜")
        return False

    try:
        # é¦–å…ˆç¡®ä¿æ‰€æœ‰ä»“åº“éƒ½æœ‰rankå­—æ®µ
        for i, repo in enumerate(repos):
            repo['rank'] = i + 1

        # ç¡®å®šå­—æ®µé¡ºåº
        field_order = [
            'rank', 'full_name', 'url', 'description', 'stars', 'forks',
            'language', 'created_at', 'updated_at', 'last_commit_date',
            'open_issues', 'topics', 'license', 'readme_summary', 'has_readme',
            'top_contributor', 'contributor_count', 'recent_commits', 'activity_score'
        ]

        # æ”¶é›†æ‰€æœ‰å®é™…å­˜åœ¨çš„å­—æ®µ
        all_fields = set()
        for repo in repos:
            all_fields.update(repo.keys())

        # ç¡®ä¿rankåœ¨field_orderä¸­
        if 'rank' not in field_order:
            field_order.insert(0, 'rank')

        # æ’åºå­—æ®µï¼šå…ˆfield_orderä¸­çš„å­—æ®µï¼Œç„¶åå…¶ä»–å­—æ®µ
        fieldnames = [f for f in field_order if f in all_fields]
        other_fields = sorted([f for f in all_fields if f not in field_order])
        fieldnames.extend(other_fields)

        # å†™å…¥CSV
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(repos)

        print(f"\nğŸ’¾ æ•°æ®ä¿å­˜æˆåŠŸ!")
        print(f"  æ–‡ä»¶: {filename}")
        print(f"  è®°å½•æ•°: {len(repos)}")
        print(f"  å­—æ®µæ•°: {len(fieldnames)}")
        print(f"  å­—æ®µåˆ—è¡¨: {', '.join(fieldnames)}")

        return True

    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

# ==================== ä¸»ç¨‹åº ====================
def main():
    print("ğŸ¤– æ™ºèƒ½GitHubä»“åº“çˆ¬è™«ç³»ç»Ÿ")
    print("=" * 60)
    print("ç‰¹ç‚¹:")
    print("  â€¢ æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œè‡ªåŠ¨é€‚åº”APIé™åˆ¶")
    print("  â€¢ åŠ¨æ€å»¶è¿Ÿè°ƒæ•´ï¼Œé¿å…è§¦å‘é¢‘ç‡é™åˆ¶")
    print("  â€¢ æ™ºèƒ½é”™è¯¯æ¢å¤ï¼Œæ–­ç‚¹ç»­ä¼ èƒ½åŠ›")
    print("  â€¢ è¯¦ç»†çŠ¶æ€ç›‘æ§ï¼Œå®æ—¶è¿›åº¦æ˜¾ç¤º")
    print("=" * 60)

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = SmartGitHubCrawler()

        # å¼€å§‹çˆ¬å–
        start_time = time.time()
        repositories = crawler.crawl_intelligently()

        if repositories:
            # ä¿å­˜ç»“æœ
            success = save_repositories_to_csv(repositories, config.OUTPUT_FILE)

            if success:
                # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
                total_time = time.time() - start_time
                print("\n" + "=" * 60)
                print("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ!")
                print("=" * 60)
                print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
                print(f"  è·å–ä»“åº“æ•°: {len(repositories)} ä¸ª")
                print(f"  æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time / 60:.1f}åˆ†é’Ÿ)")
                print(f"  å¹³å‡é€Ÿåº¦: {len(repositories) / (total_time / 60):.1f} ä¸ª/åˆ†é’Ÿ")

                # æ˜¾ç¤ºAPIä½¿ç”¨æ€»ç»“
                final_status = crawler.api.get_api_status()
                print(f"\nğŸ“¡ APIä½¿ç”¨æ€»ç»“:")
                print(f"  æœç´¢APIä½¿ç”¨: {final_status.get('search_used', 0)}/30 æ¬¡")
                print(f"  æ ¸å¿ƒAPIä½¿ç”¨: {final_status.get('core_used', 0)}/5000 æ¬¡")
                print(f"  æœ€ç»ˆè¯·æ±‚å»¶è¿Ÿ: {final_status.get('current_delay', 0):.1f}ç§’")

        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•ä»“åº“æ•°æ®")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºè¿è¡Œå¼‚å¸¸: {type(e).__name__}: {e}")


if __name__ == '__main__':
    main()