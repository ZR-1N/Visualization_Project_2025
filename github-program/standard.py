import json
import re
from datetime import datetime
from collections import Counter, defaultdict
import math


def clean_and_normalize_data(input_file, output_file):
    """
    æ¸…æ´—ã€æ ‡å‡†åŒ–GitHubæ•°æ®ï¼Œç”Ÿæˆé€‚åˆå¯è§†åŒ–çš„ç»“æ„
    """
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"ğŸ“Š è¯»å–åˆ° {len(data)} æ¡æ•°æ®")

    # 1. æ•°æ®æ¸…æ´—
    cleaned_data = []
    for item in data:
        # åˆ›å»ºæ¸…æ´—åçš„é¡¹ç›®å­—å…¸
        clean_item = {}

        # åŸºæœ¬å­—æ®µå¤„ç†
        clean_item['rank'] = int(item.get('rank', 0))
        clean_item['full_name'] = item.get('full_name', '').strip()
        clean_item['url'] = item.get('url', '')
        clean_item['description'] = item.get('description', '').strip()

        # æ•°å€¼å­—æ®µè½¬æ¢
        clean_item['stars'] = parse_number(item.get('stars', '0'))
        clean_item['forks'] = parse_number(item.get('forks', '0'))
        clean_item['open_issues'] = parse_number(item.get('open_issues', '0'))
        clean_item['activity_score'] = parse_float(item.get('activity_score', '0'))
        clean_item['contributor_count'] = parse_number(item.get('contributor_count', '0'))
        clean_item['recent_commits'] = parse_number(item.get('recent_commits', '0'))

        # è¯­è¨€å¤„ç†
        language = item.get('language', 'æœªçŸ¥').strip()
        clean_item['language'] = normalize_language(language)

        # æ—¥æœŸå¤„ç†
        clean_item['created_at'] = parse_date(item.get('created_at', ''))
        clean_item['updated_at'] = parse_date(item.get('updated_at', ''))
        clean_item['last_commit_date'] = parse_date(item.get('last_commit_date', ''))

        # è®¸å¯è¯å¤„ç†
        license_text = item.get('license', 'æ— ').strip()
        clean_item['license'] = normalize_license(license_text)

        # ä¸»é¢˜æ ‡ç­¾å¤„ç†
        topics_str = item.get('topics', '')
        clean_item['topics'] = parse_topics(topics_str)

        # å…¶ä»–å­—æ®µ
        clean_item['has_readme'] = item.get('has_readme', 'False') == 'True'
        clean_item['top_contributor'] = item.get('top_contributor', '').strip()
        clean_item['readme_summary'] = clean_text_summary(item.get('readme_summary', ''))

        # è®¡ç®—è¡ç”Ÿå­—æ®µ
        clean_item['age_days'] = calculate_age_days(clean_item['created_at'])
        clean_item['stars_per_day'] = calculate_stars_per_day(clean_item['stars'], clean_item['age_days'])
        clean_item['forks_per_star'] = calculate_forks_per_star(clean_item['forks'], clean_item['stars'])
        clean_item['is_active'] = clean_item['activity_score'] >= 70

        cleaned_data.append(clean_item)

    print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼Œ{len(cleaned_data)} æ¡æœ‰æ•ˆæ•°æ®")

    # 2. ç”Ÿæˆç»“æ„åŒ–æ•°æ®ç”¨äºå¯è§†åŒ–
    structured_data = {
        # åŸå§‹æ•°æ®ï¼ˆæ¸…æ´—åï¼‰
        'projects': cleaned_data,

        # æ±‡æ€»ç»Ÿè®¡
        'summary_stats': generate_summary_stats(cleaned_data),

        # è¯­è¨€åˆ†æ
        'language_analysis': analyze_languages(cleaned_data),

        # æ—¶é—´è¶‹åŠ¿
        'time_analysis': analyze_time_trends(cleaned_data),

        # è®¸å¯è¯åˆ†æ
        'license_analysis': analyze_licenses(cleaned_data),

        # ä¸»é¢˜åˆ†æ
        'topic_analysis': analyze_topics(cleaned_data),

        # æ´»è·ƒåº¦åˆ†æ
        'activity_analysis': analyze_activity(cleaned_data),

        # ç›¸å…³æ€§åˆ†æ
        'correlation_analysis': analyze_correlations(cleaned_data),

        # Topæ’è¡Œæ¦œ
        'top_lists': generate_top_lists(cleaned_data)
    }

    # 3. ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(structured_data, f, indent=2, ensure_ascii=False, default=str)

    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

    # 4. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print_stats(structured_data)

    return structured_data


def parse_number(value):
    """è§£ææ•°å­—å­—ç¬¦ä¸²"""
    if isinstance(value, (int, float)):
        return int(value)
    if isinstance(value, str):
        # ç§»é™¤é€—å·ç­‰éæ•°å­—å­—ç¬¦
        cleaned = re.sub(r'[^\d\.]', '', value)
        try:
            return int(float(cleaned)) if cleaned else 0
        except:
            return 0
    return 0


def parse_float(value):
    """è§£ææµ®ç‚¹æ•°"""
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        try:
            return float(value)
        except:
            return 0.0
    return 0.0


def normalize_language(language):
    """æ ‡å‡†åŒ–ç¼–ç¨‹è¯­è¨€åç§°"""
    language_map = {
        'æœªçŸ¥': 'Unknown',
        'æ— ': 'Unknown',
        'Markdown': 'Markdown',
        'TypeScript': 'TypeScript',
        'Python': 'Python',
        'JavaScript': 'JavaScript',
        'Java': 'Java',
        'C++': 'C++',
        'C': 'C',
        'Go': 'Go',
        'Rust': 'Rust',
        'HTML': 'HTML',
        'CSS': 'CSS',
        'Shell': 'Shell',
        'Dart': 'Dart',
        'MDX': 'MDX',
        'Batchfile': 'Batchfile',
        'Jupyter Notebook': 'Jupyter Notebook',
        'Clojure': 'Clojure',
        'Vim Script': 'Vim Script',
        'Vue': 'Vue',
        'Svelte': 'Svelte',
        'Zig': 'Zig',
        'Blade': 'Blade',
        'Dockerfile': 'Dockerfile',
    }

    lang = language.strip()
    return language_map.get(lang, lang)


def parse_date(date_str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    if not date_str:
        return None

    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
    formats = ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y']

    for fmt in formats:
        try:
            return datetime.strptime(date_str, fmt).date().isoformat()
        except:
            continue

    # å¦‚æœéƒ½æ— æ³•è§£æï¼Œå°è¯•æå–å¹´ä»½
    year_match = re.search(r'\d{4}', date_str)
    if year_match:
        year = year_match.group()
        return f"{year}-01-01"

    return None


def normalize_license(license_text):
    """æ ‡å‡†åŒ–è®¸å¯è¯åç§°"""
    if not license_text or license_text == 'æ— ':
        return 'Unknown'

    # å¸¸è§è®¸å¯è¯æ˜ å°„
    license_map = {
        'MIT License': 'MIT',
        'Apache License 2.0': 'Apache-2.0',
        'BSD 3-Clause "New" or "Revised" License': 'BSD-3-Clause',
        'GNU General Public License v3.0': 'GPL-3.0',
        'GNU Affero General Public License v3.0': 'AGPL-3.0',
        'Creative Commons Zero v1.0 Universal': 'CC0-1.0',
        'Creative Commons Attribution 4.0 International': 'CC-BY-4.0',
        'Creative Commons Attribution Share Alike 4.0 International': 'CC-BY-SA-4.0',
        'The Unlicense': 'Unlicense',
        'ISC License': 'ISC',
        'Mozilla Public License 2.0': 'MPL-2.0',
        'SIL Open Font License 1.1': 'OFL-1.1',
    }

    # æŸ¥æ‰¾åŒ¹é…çš„è®¸å¯è¯
    for key, value in license_map.items():
        if key in license_text:
            return value

    # ç®€åŒ–å…¶ä»–è®¸å¯è¯
    if 'GNU' in license_text:
        return 'GPL Family'
    elif 'Creative Commons' in license_text:
        return 'CC Family'
    elif 'BSD' in license_text:
        return 'BSD Family'

    return 'Other'


def parse_topics(topics_str):
    """è§£æä¸»é¢˜æ ‡ç­¾"""
    if not topics_str:
        return []

    # åˆ†å‰²é€—å·åˆ†éš”çš„æ ‡ç­¾
    topics = [t.strip() for t in topics_str.split(',') if t.strip()]

    # è¿‡æ»¤ç©ºå€¼å’Œè¿‡é•¿çš„æ ‡ç­¾
    filtered = []
    for topic in topics:
        if topic and len(topic) <= 50:
            filtered.append(topic)

    return filtered


def clean_text_summary(text):
    """æ¸…ç†æ–‡æœ¬æ‘˜è¦"""
    if not text:
        return ""

    # ç§»é™¤HTMLæ ‡ç­¾
    cleaned = re.sub(r'<[^>]+>', '', text)

    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    cleaned = ' '.join(cleaned.split())

    # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
    if len(cleaned) > 200:
        cleaned = cleaned[:197] + '...'

    return cleaned


def calculate_age_days(created_date):
    """è®¡ç®—é¡¹ç›®å¹´é¾„ï¼ˆå¤©ï¼‰"""
    if not created_date:
        return 0

    try:
        created = datetime.strptime(created_date, '%Y-%m-%d').date()
        today = datetime.now().date()
        return (today - created).days
    except:
        return 0


def calculate_stars_per_day(stars, age_days):
    """è®¡ç®—æ¯æ—¥å¹³å‡æ˜Ÿæ ‡æ•°"""
    if age_days <= 0:
        return 0
    return round(stars / age_days, 4)


def calculate_forks_per_star(forks, stars):
    """è®¡ç®—æ¯æ˜Ÿæ ‡å¯¹åº”çš„åˆ†æ”¯æ•°"""
    if stars <= 0:
        return 0
    return round(forks / stars, 4)


def generate_summary_stats(data):
    """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
    stats = {
        'total_projects': len(data),
        'total_stars': sum(p['stars'] for p in data),
        'total_forks': sum(p['forks'] for p in data),
        'total_issues': sum(p['open_issues'] for p in data),
        'avg_stars': round(sum(p['stars'] for p in data) / len(data)),
        'avg_forks': round(sum(p['forks'] for p in data) / len(data)),
        'avg_activity_score': round(sum(p['activity_score'] for p in data) / len(data), 2),
        'active_projects': sum(1 for p in data if p['is_active']),
        'inactive_projects': sum(1 for p in data if not p['is_active']),
        'avg_age_days': round(sum(p['age_days'] for p in data) / len(data)),
        'oldest_project': max(data, key=lambda x: x['age_days'])['full_name'],
        'newest_project': min(data, key=lambda x: x['age_days'])['full_name'],
    }
    return stats


def analyze_languages(data):
    """åˆ†æç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ"""
    language_counter = Counter(p['language'] for p in data)
    language_stats = []

    for lang, count in language_counter.most_common():
        lang_projects = [p for p in data if p['language'] == lang]
        total_stars = sum(p['stars'] for p in lang_projects)
        avg_stars = round(total_stars / count) if count > 0 else 0
        avg_activity = round(sum(p['activity_score'] for p in lang_projects) / count, 2)

        language_stats.append({
            'language': lang,
            'count': count,
            'percentage': round(count / len(data) * 100, 2),
            'total_stars': total_stars,
            'avg_stars': avg_stars,
            'avg_activity_score': avg_activity,
            'top_project': max(lang_projects, key=lambda x: x['stars'])['full_name'] if lang_projects else ''
        })

    return sorted(language_stats, key=lambda x: x['count'], reverse=True)


def analyze_time_trends(data):
    """åˆ†ææ—¶é—´è¶‹åŠ¿"""
    # æŒ‰åˆ›å»ºå¹´ä»½åˆ†ç»„
    year_data = defaultdict(list)
    for project in data:
        if project['created_at']:
            year = project['created_at'][:4]
            year_data[year].append(project)

    # ç”Ÿæˆå¹´åº¦ç»Ÿè®¡
    yearly_stats = []
    for year in sorted(year_data.keys()):
        projects = year_data[year]
        yearly_stats.append({
            'year': int(year),
            'count': len(projects),
            'total_stars': sum(p['stars'] for p in projects),
            'avg_stars': round(sum(p['stars'] for p in projects) / len(projects)),
            'avg_activity': round(sum(p['activity_score'] for p in projects) / len(projects), 2)
        })

    # è®¡ç®—æ¯æœˆåˆ›å»ºæ•°ï¼ˆæœ€è¿‘3å¹´ï¼‰
    monthly_data = defaultdict(int)
    recent_projects = [p for p in data if p['created_at'] and int(p['created_at'][:4]) >= 2020]

    for project in recent_projects:
        month_key = project['created_at'][:7]  # YYYY-MM
        monthly_data[month_key] += 1

    monthly_stats = [{'month': month, 'count': count}
                     for month, count in sorted(monthly_data.items())]

    return {
        'yearly': yearly_stats,
        'monthly': monthly_stats,
        'oldest_project_year': min(yearly_stats, key=lambda x: x['year'])['year'] if yearly_stats else None,
        'newest_project_year': max(yearly_stats, key=lambda x: x['year'])['year'] if yearly_stats else None
    }


def analyze_licenses(data):
    """åˆ†æè®¸å¯è¯åˆ†å¸ƒ"""
    license_counter = Counter(p['license'] for p in data)
    license_stats = []

    for lic, count in license_counter.most_common():
        lic_projects = [p for p in data if p['license'] == lic]
        total_stars = sum(p['stars'] for p in lic_projects)

        license_stats.append({
            'license': lic,
            'count': count,
            'percentage': round(count / len(data) * 100, 2),
            'total_stars': total_stars,
            'avg_stars': round(total_stars / count) if count > 0 else 0,
            'top_project': max(lic_projects, key=lambda x: x['stars'])['full_name'] if lic_projects else ''
        })

    return license_stats


def analyze_topics(data):
    """åˆ†æä¸»é¢˜æ ‡ç­¾"""
    all_topics = []
    for project in data:
        all_topics.extend(project['topics'])

    topic_counter = Counter(all_topics)

    # çƒ­é—¨ä¸»é¢˜
    hot_topics = []
    for topic, count in topic_counter.most_common(30):  # å–å‰30
        hot_topics.append({
            'topic': topic,
            'count': count,
            'percentage': round(count / len(data) * 100, 2)
        })

    # ä¸»é¢˜å…³è”æ€§ï¼ˆç®€å•çš„å…±ç°åˆ†æï¼‰
    topic_cooccurrence = defaultdict(int)
    for project in data:
        topics = project['topics']
        for i in range(len(topics)):
            for j in range(i + 1, len(topics)):
                pair = tuple(sorted([topics[i], topics[j]]))
                topic_cooccurrence[pair] += 1

    # å–æœ€å¸¸è§çš„ä¸»é¢˜å¯¹
    top_pairs = sorted(topic_cooccurrence.items(), key=lambda x: x[1], reverse=True)[:20]
    cooccurrence_stats = [{
        'topic1': pair[0],
        'topic2': pair[1],
        'count': count
    } for pair, count in top_pairs]

    return {
        'hot_topics': hot_topics,
        'cooccurrence': cooccurrence_stats,
        'total_unique_topics': len(topic_counter),
        'avg_topics_per_project': round(len(all_topics) / len(data), 2)
    }


def analyze_activity(data):
    """åˆ†ææ´»è·ƒåº¦"""
    # æ´»è·ƒåº¦åˆ†å¸ƒ
    activity_bins = defaultdict(int)
    for project in data:
        score = project['activity_score']
        if score >= 90:
            activity_bins['90-100'] += 1
        elif score >= 80:
            activity_bins['80-89'] += 1
        elif score >= 70:
            activity_bins['70-79'] += 1
        elif score >= 60:
            activity_bins['60-69'] += 1
        else:
            activity_bins['0-59'] += 1

    activity_distribution = [{'range': k, 'count': v, 'percentage': round(v / len(data) * 100, 2)}
                             for k, v in activity_bins.items()]

    # æœ€è¿‘æ›´æ–°åˆ†æ
    recent_updates = [p for p in data if p['updated_at']]
    recent_updates.sort(key=lambda x: x['updated_at'], reverse=True)

    latest_projects = [{
        'rank': p['rank'],
        'full_name': p['full_name'],
        'updated_at': p['updated_at'],
        'activity_score': p['activity_score']
    } for p in recent_updates[:10]]

    return {
        'distribution': activity_distribution,
        'latest_updated': latest_projects,
        'high_activity_projects': sum(1 for p in data if p['activity_score'] >= 80),
        'low_activity_projects': sum(1 for p in data if p['activity_score'] < 60)
    }


def analyze_correlations(data):
    """åˆ†æç›¸å…³æ€§"""
    # æ˜Ÿæ ‡ä¸åˆ†æ”¯ç›¸å…³æ€§æ•°æ®
    stars_forks_data = [{
        'full_name': p['full_name'],
        'stars': p['stars'],
        'forks': p['forks'],
        'language': p['language']
    } for p in data]

    # æ˜Ÿæ ‡ä¸æ´»è·ƒåº¦ç›¸å…³æ€§æ•°æ®
    stars_activity_data = [{
        'full_name': p['full_name'],
        'stars': p['stars'],
        'activity_score': p['activity_score'],
        'is_active': p['is_active']
    } for p in data]

    # è¯­è¨€ä¸æ˜Ÿæ ‡å…³ç³»
    language_stars = defaultdict(list)
    for p in data:
        language_stars[p['language']].append(p['stars'])

    language_avg_stars = [{
        'language': lang,
        'avg_stars': round(sum(stars) / len(stars)),
        'max_stars': max(stars),
        'min_stars': min(stars)
    } for lang, stars in language_stars.items() if len(stars) >= 5]

    # è®¡ç®—ç›¸å…³ç³»æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if len(data) > 1:
        stars_values = [p['stars'] for p in data]
        forks_values = [p['forks'] for p in data]
        activity_values = [p['activity_score'] for p in data]

        # è®¡ç®—Pearsonç›¸å…³ç³»æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        def simplified_corr(x, y):
            if len(x) != len(y):
                return 0
            n = len(x)
            mean_x = sum(x) / n
            mean_y = sum(y) / n

            numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))
            denominator_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))
            denominator_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))

            if denominator_x == 0 or denominator_y == 0:
                return 0

            return round(numerator / (denominator_x * denominator_y), 3)

        stars_forks_corr = simplified_corr(stars_values, forks_values)
        stars_activity_corr = simplified_corr(stars_values, activity_values)
    else:
        stars_forks_corr = 0
        stars_activity_corr = 0

    return {
        'stars_vs_forks': {
            'data': stars_forks_data,
            'correlation': stars_forks_corr
        },
        'stars_vs_activity': {
            'data': stars_activity_data,
            'correlation': stars_activity_corr
        },
        'language_stars': sorted(language_avg_stars, key=lambda x: x['avg_stars'], reverse=True),
        'interpretation': {
            'stars_forks': "æ­£ç›¸å…³è¡¨ç¤ºæ˜Ÿæ ‡å¤šçš„é¡¹ç›®é€šå¸¸åˆ†æ”¯ä¹Ÿå¤š",
            'stars_activity': "æ­£ç›¸å…³è¡¨ç¤ºæµè¡Œçš„é¡¹ç›®é€šå¸¸æ›´æ´»è·ƒ"
        }
    }


def generate_top_lists(data):
    """ç”Ÿæˆå„ç§Topæ’è¡Œæ¦œ"""
    # æŒ‰æ˜Ÿæ ‡æ’åº
    top_stars = sorted(data, key=lambda x: x['stars'], reverse=True)[:20]

    # æŒ‰æ´»è·ƒåº¦æ’åº
    top_activity = sorted(data, key=lambda x: x['activity_score'], reverse=True)[:20]

    # æŒ‰æ¯æ—¥æ˜Ÿæ ‡å¢é•¿æ’åºï¼ˆçƒ­é—¨é¡¹ç›®ï¼‰
    top_growth = [p for p in data if p['stars_per_day'] > 0]
    top_growth = sorted(top_growth, key=lambda x: x['stars_per_day'], reverse=True)[:20]

    # æŒ‰åˆ†æ”¯/æ˜Ÿæ ‡æ¯”æ’åºï¼ˆé«˜å‚ä¸åº¦ï¼‰
    top_engagement = [p for p in data if p['stars'] > 1000]
    top_engagement = sorted(top_engagement, key=lambda x: x['forks_per_star'], reverse=True)[:20]

    # æŒ‰é—®é¢˜æ•°é‡æ’åºï¼ˆéœ€è¦ç»´æŠ¤çš„é¡¹ç›®ï¼‰
    top_issues = sorted(data, key=lambda x: x['open_issues'], reverse=True)[:20]

    return {
        'by_stars': [format_top_item(p, 'stars') for p in top_stars],
        'by_activity': [format_top_item(p, 'activity_score') for p in top_activity],
        'by_growth': [format_top_item(p, 'stars_per_day') for p in top_growth],
        'by_engagement': [format_top_item(p, 'forks_per_star') for p in top_engagement],
        'by_issues': [format_top_item(p, 'open_issues') for p in top_issues]
    }


def format_top_item(project, metric_key):
    """æ ¼å¼åŒ–æ’è¡Œæ¦œé¡¹ç›®"""
    metric_names = {
        'stars': 'Stars',
        'activity_score': 'Activity Score',
        'stars_per_day': 'Stars/Day',
        'forks_per_star': 'Forks/Star',
        'open_issues': 'Open Issues'
    }

    return {
        'rank': project['rank'],
        'full_name': project['full_name'],
        'language': project['language'],
        metric_names[metric_key]: project[metric_key],
        'url': project['url']
    }


def print_stats(structured_data):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    stats = structured_data['summary_stats']
    print("\nğŸ“ˆ æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
    print(f"   é¡¹ç›®æ€»æ•°: {stats['total_projects']}")
    print(f"   æ€»æ˜Ÿæ ‡æ•°: {stats['total_stars']:,}")
    print(f"   æ€»åˆ†æ”¯æ•°: {stats['total_forks']:,}")
    print(f"   å¹³å‡æ˜Ÿæ ‡: {stats['avg_stars']:,}")
    print(f"   å¹³å‡æ´»è·ƒåº¦: {stats['avg_activity_score']}")
    print(
        f"   æ´»è·ƒé¡¹ç›®æ•°: {stats['active_projects']} ({stats['active_projects'] / stats['total_projects'] * 100:.1f}%)")

    langs = structured_data['language_analysis'][:5]
    print(f"\nğŸ”¤ çƒ­é—¨è¯­è¨€Top 5:")
    for lang in langs:
        print(f"   {lang['language']}: {lang['count']}ä¸ªé¡¹ç›® ({lang['percentage']}%)")

    topics = structured_data['topic_analysis']['hot_topics'][:5]
    print(f"\nğŸ·ï¸  çƒ­é—¨æ ‡ç­¾Top 5:")
    for topic in topics:
        print(f"   {topic['topic']}: {topic['count']}æ¬¡å‡ºç°")


# æ‰§è¡Œæ•°æ®å¤„ç†
if __name__ == "__main__":
    input_file = "github_top_500_smart_20251206_170411.json"
    output_file = "github_processed_standardized.json"

    try:
        processed_data = clean_and_normalize_data(input_file, output_file)
        print(f"\nğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“ æ–‡ä»¶åŒ…å«ä»¥ä¸‹æ•°æ®ç»“æ„:")
        print("   - projects: æ¸…æ´—åçš„åŸå§‹é¡¹ç›®æ•°æ®")
        print("   - summary_stats: æ±‡æ€»ç»Ÿè®¡")
        print("   - language_analysis: è¯­è¨€åˆ†æ")
        print("   - time_analysis: æ—¶é—´è¶‹åŠ¿")
        print("   - license_analysis: è®¸å¯è¯åˆ†æ")
        print("   - topic_analysis: ä¸»é¢˜åˆ†æ")
        print("   - activity_analysis: æ´»è·ƒåº¦åˆ†æ")
        print("   - correlation_analysis: ç›¸å…³æ€§åˆ†æ")
        print("   - top_lists: å„ç§æ’è¡Œæ¦œ")

    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")